---
title: "DATA 621: BUSINESS ANALYTICS AND DATA MINING HOMEWORK#5 Assignment Requirements"
date: "Last edited `r format(Sys.time(), '%B %d, %Y')`"
author: "Group 2 - Gabriel Campos, Melissa Bowman, Alexander Khaykin, & Jennifer Abinette"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 4
    number_sections: true
    highlight: tango
  geometry: "left=0.5cm,right=0.5cm,top=0.5cm,bottom=0.5cm"

urlcolor: blue
---

**Overview**

&emsp; In this homework assignment, you will explore, analyze and model a data set containing information on
approximately 12,000 commercially available wines. The variables are mostly related to the chemical properties of
the wine being sold. The response variable is the number of sample cases of wine that were purchased by wine
distribution companies after sampling a wine. These cases would be used to provide tasting samples to
restaurants and wine stores around the United States. The more sample cases purchased, the more likely is a
wine to be sold at a high end restaurant. A large wine manufacturer is studying the data in order to predict the
number of wine cases ordered based upon the wine characteristics. If the wine manufacturer can predict the
number of cases, then that manufacturer will be able to adjust their wine offering to maximize sales.

&emsp; Your objective is to build a count regression model to predict the number of cases of wine that will be sold
given certain properties of the wine. HINT: Sometimes, the fact that a variable is missing is actually predictive of
the target. You can only use the variables given to you (or variables that you derive from the variables provided).
Below is a short description of the variables of interest in the data set:
<br>

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library('dplyr')
library('tidyverse')

library('psych')

#Added by Melissa 
library(ggplot2)
library(MASS)   # For negative binomial regression
library(pscl)   # For hurdle and zero-inflated models
library(Metrics) # For model evaluation metrics
library(skimr)
```

**Deliverables**

* A write-up submitted in PDF format. Your write-up should have four sections. Each one is described below. You may assume you are addressing me as a fellow data scientist, so do not need to shy away from technical details.
* Assigned predictions (number of cases of wine sold) for the evaluation data set.
* Include your R statistical programming code in an Appendix.

**Write Up:**

1. DATA EXPLORATION (25 Points)

Describe the size and the variables in the wine training data set. Consider that too much detail will cause a
manager to lose interest while too little detail will make the manager consider that you aren’t doing your job. Some
suggestions are given below. Please do NOT treat this as a check list of things to do to complete the assignment.
You should have your own thoughts on what to tell the boss. These are just ideas.

a. Mean / Standard Deviation / Median
b. Bar Chart or Box Plot of the data
c. Is the data correlated to the target variable (or to other variables?)
d. Are any of the variables missing and need to be imputed “fixed”?

2. DATA PREPARATION (25 Points)

Describe how you have transformed the data by changing the original variables or creating new variables. If you did transform the data or create new variables, discuss why you did this. Here are some possible transformations.

a. Fix missing values (maybe with a Mean or Median value)
b. Create flags to suggest if a variable was missing
c. Transform data by putting it into buckets
d. Mathematical transforms such as log or square root (or use Box-Cox)
e. Combine variables (such as ratios or adding or multiplying) to create new variables

3. BUILD MODELS (25 Points)

Using the training data set, build at least two different poisson regression models, at least two different negative binomial regression models, and at least two multiple linear regression models, using different variables (or the same variables with different transformations). Sometimes poisson and negative binomial regression models give the same results. If that is the case, comment on that. Consider changing the input variables if that occurs so that you get different models. Although not covered in class, you may also want to consider building zero-inflated poisson and negative binomial regression models. You may select the variables manually, use an approach such as Forward or Stepwise, use a different approach such as trees, or use a combination of techniques. Describe the techniques you used. If you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done 

Discuss the coefficients in the models, do they make sense? In this case, about the only thing you can comment on is the number of stars and the wine label appeal. However, you might comment on the coefficient and magnitude of variables and how they are similar or different from model to model. For example, you might say “pH seems to have a major positive impact in my poisson regression model, but a negative effect in my multiple linear regression model”. Are you keeping the model even though it is counter intuitive? Why? The boss needs to know.

4. SELECT MODELS (25 Points)

Decide on the criteria for selecting the best count regression model. Will you select models with slightly worse performance if it makes more sense or is more parsimonious? Discuss why you selected your models.

For the count regression model, will you use a metric such as AIC, average squared error, etc.? Be sure to explain how you can make inferences from the model, and discuss other relevant model output. If you like the multiple linear regression model the best, please say why. However, you must select a count regression model for model deployment. Using the training data set, evaluate the performance of the count regression model. Make predictions using the evaluation data set.

\newpage

# **DATA EXPLORATION**

Import Data

```{r, echo=FALSE}
url_git <- "https://raw.githubusercontent.com/GitableGabe/Data621_Data/main/"
```

```{r}
df_wine_eval <- 
  read.csv(paste0(url_git,"wine-evaluation-data.csv"))

head(df_wine_eval)
```

```{r}
df_wine_train <- 
  read.csv(paste0(url_git,"wine-training-data.csv"))

head(df_wine_train)
```

## Evaluation Data set

The evaluation data set contains 3,335 observations and 16 variables, although the Target variable is currently missing all values as we will predict those later once we choose a model.

### Summary Statistics

```{r, warning=FALSE}
dim(df_wine_eval)
```

```{r, warning=FALSE}
describe(df_wine_eval)
```

```{r}
summary(df_wine_eval)
```

```{r}
str(df_wine_eval)
```

### Missing Data

```{r}
for (i in colnames(df_wine_eval)){
  print(paste(i,"  ", sum(is.na(df_wine_eval[,i])),sep = ""))
}
```

### Outliers

```{r, warning=FALSE}
df_wine_eval %>%
  scale() %>%
  as.data.frame() %>%
  stack() %>%
  ggplot(aes(x = ind, y = values)) +
  geom_boxplot() +
  labs(title = 'Boxplot Eval (scaled)',
       x = 'Variables',
       y = 'Normalized_Values')+
  theme(axis.text.x=element_text(size=10, angle=90)) 
```


## Training Data set

The training data set contains 12,795 observations and 16 variables with our response variable (TARGET) indicating the number of wine cases purchased which ranges from 0 to 8.  Our variables include information on the content of each wine such as alcohol, citric acid, sulfur dioxide, etc. as well as a wine rating by a team of experts (STARS variable).  Our intent is to use this training data set to create the best fitted regression model so that we can predict the number of cases sold for the wines in the evaluation data set. 

### Summary Statistics

```{r, echo=FALSE, warning=FALSE}
df_wine_train_dim <-dim(df_wine_train)
```

```{r}
describe(df_wine_train)
```

```{r}
summary(df_wine_train)
```

```{r}
str(df_wine_train)
```

### Missing Data

```{r}
for (i in colnames(df_wine_train)){
  print(paste(i,"  ", sum(is.na(df_wine_train[,i])),sep = ""))
}
```

### Outliers

```{r, warning=FALSE}
df_wine_train %>%
  scale() %>%
  as.data.frame() %>%
  stack() %>%
  ggplot(aes(x = ind, y = values)) +
  geom_boxplot() +
  labs(title = 'Boxplot Training (scaled)',
       x = 'Variables',
       y = 'Normalized_Values')+
  theme(axis.text.x=element_text(size=10, angle=90)) 
```

### Variable Distributions

```{r}
# Gather the data into a long format
data_long <- gather(df_wine_train, key = "Variable", value = "Value")

ggplot(data_long, aes(x = Value)) +
  geom_histogram() +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Histogram of Variables")
```

**As shown above, the distribution of data is relatively normal so we do not have to transform any variables to improve their distribution.**

### Correlation between Variables

```{r}
# Create a correlation matrix for all variables
(cor_matrix <- cor(df_wine_train, use='complete.obs'))
```

```{r}
cor(df_wine_train, y=df_wine_train$TARGET)
```

**At first glance, Only two variables Label Appeal, and Acid Index have a relationship with the Target variable, number of wine cases purchased. We do see that there are several variables with missing values that we will explore transforming and then reevaluate the correlation with TARGET. Of particular interest to us is the wine rating variable (STARS) given we would predict this to be strongly correlated to the number of cases purchased.  However, the STARS variable contains 3,359 missing values out of 12,795, which is about 26% of our sample so we will need to address how to best handle this.**

### Target Variable

```{r}
hist(df_wine_train$TARGET,main="Histogram of TARGET",xlab="TARGET",col="skyblue",border="black")
```

```{r}
# Calculate the percentage of unique values in the TARGET variable
target_table <- table(df_wine_train$TARGET)
target_percentage <- prop.table(target_table) * 100


rounded_percentage <- round(target_percentage, 2)


print(rounded_percentage)
```

```{r}
df_wine_train %>% 
  mutate(STARS = as.factor(STARS),
         TARGET = as.factor(TARGET)) %>% 
  ggplot(aes(STARS)) +
  geom_bar(aes(fill = TARGET)) +
  scale_fill_brewer(palette = "RdYlGn") 
```

# **DATA PREPARATION**

## Dealing with Missing Values
**As demonstrated above, STARS has a lot of NA values that relate to a TARGET value of 0 so removing all cases that have missing values in STARS would negatively impact our model.  Alternatively, we can impute zero for every missing value in the STARS variable instead of eliminating 26% of our cases by dropping NA values**

### Zero Imputation for All

```{r}
df_wine_train_zero <- df_wine_train %>% 
  mutate(STARS = replace(STARS, is.na(STARS) , 0)) %>%
  mutate(ResidualSugar = replace(ResidualSugar, is.na(ResidualSugar) , 0)) %>%
  mutate(Chlorides = replace(Chlorides, is.na(Chlorides) , 0)) %>%
  mutate(FreeSulfurDioxide = replace(FreeSulfurDioxide, is.na(FreeSulfurDioxide) , 0)) %>%
  mutate(TotalSulfurDioxide = replace(TotalSulfurDioxide, is.na(TotalSulfurDioxide) , 0)) %>%
  mutate(Density = replace(Density, is.na(Density) , 0)) %>%
  mutate(pH = replace(pH, is.na(pH) , 0)) %>%
  mutate(Sulphates = replace(Sulphates, is.na(Sulphates) , 0)) %>%
  mutate(Alcohol = replace(Alcohol, is.na(Alcohol) , 0))
```

```{r}
df_na_transformations1 <- subset(df_wine_train_zero, select=c("TARGET","STARS","ResidualSugar","Chlorides","FreeSulfurDioxide",
                                                                     "TotalSulfurDioxide","Density","pH","Sulphates","Alcohol"))
cor(df_na_transformations1, y=df_na_transformations1$TARGET)
```

**As predicted, STARS has a strong positive relationship with the number of cases purchased.  Meanwhile, the other variables we used zero imputation on have weak relationships. Let's consider removing the cases with missing values for the other variables besides STARS.**

### Removing Cases with NA values from all other variables 

```{r}
df_wine_train_zero_removed <- df_wine_train %>% 
  mutate(STARS = replace(STARS, is.na(STARS) , 0)) %>%
  na.omit()
```

```{r}
df_na_transformations2 <- subset(df_wine_train_zero_removed, select=c("TARGET","STARS","ResidualSugar","Chlorides","FreeSulfurDioxide",
                                                                     "TotalSulfurDioxide","Density","pH","Sulphates","Alcohol"))
cor(df_na_transformations2, y=df_na_transformations2$TARGET)
```
**If we choose to remove cases with missing values (except in STARS variable) then we cut our sample down from 12,795 to 8,675.  Given that the removal of cases with NA does not appear to affect the correlations with the response variable, we will instead keep all cases & use mean imputation.**

### Zero for STARS & Mean Imputation for all other variables 

```{r}
# Get the Means of columns in Data
train_means<-sapply(df_wine_train, function(x) round(mean(x, na.rm = TRUE)))

df_wine_train_mean <- df_wine_train %>% 
  mutate(STARS = replace(STARS, is.na(STARS) , 0)) %>%
# Replace other NA values in 'column_name' with 'mean'
  mutate(ResidualSugar = replace(ResidualSugar, is.na(ResidualSugar) , train_means[6])) %>%
  mutate(Chlorides = replace(Chlorides, is.na(Chlorides) , train_means[7])) %>%
  mutate(FreeSulfurDioxide = replace(FreeSulfurDioxide, is.na(FreeSulfurDioxide) , train_means[8])) %>%
  mutate(TotalSulfurDioxide = replace(TotalSulfurDioxide, is.na(TotalSulfurDioxide) , train_means[9])) %>%
  mutate(Density = replace(Density, is.na(Density) , train_means[10])) %>%
  mutate(pH = replace(pH, is.na(pH) , train_means[11])) %>%
  mutate(Sulphates = replace(Sulphates, is.na(Sulphates) , train_means[12])) %>%
  mutate(Alcohol = replace(Alcohol, is.na(Alcohol) , train_means[13]))

```

```{r}
df_na_transformations3 <- subset(df_wine_train_mean, select=c("TARGET","STARS","ResidualSugar","Chlorides","FreeSulfurDioxide",
                                                                     "TotalSulfurDioxide","Density","pH","Sulphates","Alcohol"))
cor(df_na_transformations3, y=df_na_transformations3$TARGET)
```

**Besides STARS, the different imputations saw little to no improvement for the relationship with TARGET.  As it stands, cases of wine purchased appears to have meaningful relationships to STARS, Label Appeal, and Acid Index. Taking a further look at the other variables, we see many variables that have negative values where we would not expect, such as alcohol content. Let's consider transforming these variables given negative values could negatively impact our models and it does not make sense to have negative content.  Additionally, we will create indicators for alcohol content and another for strong acidity (pH less than 3) as could be better predictors than the exact numeric value.**

## Transformations and New Variables

```{r}

df_wine_train_transformed <- subset(df_wine_train, select =-INDEX) %>% 
  mutate(STARS = replace(STARS, is.na(STARS) , 0)) %>%
# Replace missing or negative values with zero
  mutate(FixedAcidity = replace(FixedAcidity, is.na(FixedAcidity)|FixedAcidity <0  , 0)) %>%
  mutate(VolatileAcidity = replace(VolatileAcidity, is.na(VolatileAcidity)|VolatileAcidity <0  , 0)) %>%
  mutate(CitricAcid = replace(CitricAcid, is.na(CitricAcid)|CitricAcid <0  , 0)) %>%
  mutate(ResidualSugar = replace(ResidualSugar, is.na(ResidualSugar)|ResidualSugar <0 , 0)) %>%
  mutate(Chlorides = replace(Chlorides, is.na(Chlorides)|Chlorides <0  , 0)) %>%
  mutate(FreeSulfurDioxide = replace(FreeSulfurDioxide, is.na(FreeSulfurDioxide)|FreeSulfurDioxide <0  , 0)) %>%
  mutate(TotalSulfurDioxide = replace(TotalSulfurDioxide, is.na(TotalSulfurDioxide)|TotalSulfurDioxide <0  , 0)) %>%
  mutate(Density = replace(Density, is.na(Density)|Density <0  , 0)) %>%
  mutate(Sulphates = replace(Sulphates, is.na(Sulphates)|Sulphates <0 , 0)) %>%
  mutate(Alcohol = replace(Alcohol, is.na(Alcohol)|Alcohol <0 , 0)) %>%
# pH values can be negative so we will only impute zero for missing values
  mutate(pH = replace(pH, is.na(pH), 0)) %>%

# Create new variables
  mutate(Alcohol_ind = ifelse(Alcohol == 0, 0 , 1)) %>%
  mutate(pH_acidic = ifelse(pH > 0 & pH < 3, 1, 0))

```

```{r}
cor(df_wine_train_transformed, y=df_wine_train_transformed$TARGET)
```

**Since there are an excess of zero values in the data set, the Poisson and Negative Binomial Regression may not be able to give the best model outcome. Therefore, we will also test Hurdle Poisson and Zero-Inflated Poisson Regression models to see if these models work best. To compare these models, we will be using the The Root Mean Squared Error (RMSE). The lowest number will tell us which model works best.**

## Train-test split
```{r}
set.seed(100)  
n <- nrow(df_wine_train_transformed)
train_index <- sample(1:n, 0.8 * n)  # 80% for training, 20% for testing
df_train <- df_wine_train_transformed[train_index, ]
df_test <- df_wine_train_transformed[-train_index, ]
```

# **BUILD MODELS**

**We will be exploring various types of regression models including Multiple Linear, Poisson, and Negative Binomial.**

## Multiple Linear Regression

### Model 1 - Backward Elimination

```{r}
MLR_model_all <- lm(TARGET ~ ., data = df_train)
MLR_step <- step(MLR_model_all, direction = "backward", test = "F")
```

**After backwards elimination, our model contains 12 variables.  However, we will additionally remove CitricAcid for having a p-value greater than .05 and the Alcohol Indicator as the Alcohol variable is also in the model with a lower p-value.**

```{r}
MLR_model_back <- lm(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + 
    TotalSulfurDioxide + Density + Sulphates + Alcohol + LabelAppeal + 
    AcidIndex + STARS, data = df_train)
summary(MLR_model_back)
```

### Prediction of test-split data
```{r}
MLR_back_preds <- round( predict(MLR_model_back, newdata = df_test, type = "response"))
```

### RMSE
```{r}
MLR_back_rmse <- sqrt(mean((MLR_back_preds - df_test$TARGET)^2))
```

### Model 2 - Manual Variable Selection

**We noted that the backwards elimination steps demonstrated that many of the variables, even if the coefficients are significant, do not have much impact on model fit evidenced by the AIC and Adjusted R-Square values changing very little. Also, many of these variables do not intuitively have a direct relationship with the number of cases of wine purchased. In this manner, we want to explore simpler models and focus on variables with stronger correlations with our response variable.**

```{r}
MLR_model_manual <- lm(TARGET ~ Alcohol + LabelAppeal + AcidIndex + STARS, data = df_train)
summary(MLR_model_manual)
```

### Prediction of test-split data
```{r}
MLR_manual_preds <- round( predict(MLR_model_manual, newdata = df_test, type = "response"))
```

### RMSE
```{r}
MLR_manual_rmse <- sqrt(mean((MLR_manual_preds - df_test$TARGET)^2))
```


## Poisson Regression

### Model 1 - Manual Selection
As noted above, many of the variables have weak relationships with our TARGET variable so we will choose to focus on those with a clear relationship including Label Appeal, Acid Index, and STARS.

```{r}
poisson_model <- glm(TARGET ~ LabelAppeal + AcidIndex + STARS, data = df_train, family = poisson)
summary(poisson_model)
```

#### Prediction of test-split data
```{r}
poisson_preds <- round( predict(poisson_model, newdata = df_test, type = "response"))
```

#### RMSE
```{r}
poisson_rmse <- sqrt(mean((poisson_preds - df_test$TARGET)^2))
```

### Model 2 - Hurdle Poisson Regression

```{r}
hurdle_poisson_model <- hurdle(TARGET ~ LabelAppeal + AcidIndex + STARS, data = df_train, dist = "poisson")
summary(hurdle_poisson_model)
```

#### Prediction of test-split data
```{r}
hurdle_preds <- round( predict(hurdle_poisson_model, newdata = df_test, type = "response") )
```

#### RMSE
```{r}
hurdle_rmse <- sqrt(mean((hurdle_preds - df_test$TARGET)^2))
```

### Model 3 - Zero-Inflated Poisson Regression

```{r}
zip_model <- zeroinfl(TARGET ~ LabelAppeal + AcidIndex + STARS | 1, data = df_train, dist = "poisson")
summary(zip_model)
```

#### Prediction of test-split data
```{r}
zip_preds <- round( predict(zip_model, newdata = df_test, type = "response") )
```

#### RMSE
```{r}
zip_rmse <- sqrt(mean((zip_preds - df_test$TARGET)^2))
```


## Negative Binomial Regression

### Model 1 - Strongly Correlated Variables

```{r}
neg_binom_model <- glm.nb(TARGET ~ LabelAppeal + AcidIndex + STARS, data = df_train)
summary(neg_binom_model)
```

### Prediction of test-split data
```{r}
neg_binom_preds <- round( predict(neg_binom_model, newdata = df_test, type = "response") )
```

### RMSE
```{r}
neg_binom_rmse <- sqrt(mean((neg_binom_preds - df_test$TARGET)^2))
```

### Model 2 - Include Alcohol content
The alcohol variable in previous models has increased effect size and customers may be attracted to a higher alcohol content so we will see if the addition will create a better model fit.

```{r}
neg_binom_model_alc <- glm.nb(TARGET ~ Alcohol + LabelAppeal + AcidIndex + STARS, data = df_train)
summary(neg_binom_model_alc)
```

### Prediction of test-split data
```{r}
neg_binom_alc_preds <- round( predict(neg_binom_model_alc, newdata = df_test, type = "response") )
```

### RMSE
```{r}
neg_binom_alc_rmse <- sqrt(mean((neg_binom_alc_preds - df_test$TARGET)^2))
```

# **SELECT MODELS** 

## Compare RMSE
```{r}
comparison <- data.frame(
  Model = c("MLR Backward Elimination", "MLR Manual", "Poisson", "Hurdle Poisson", 
            "Zero-Inflated Poisson","Negative Binomial 1", "Negative Binomial 2"),
  RMSE = c(MLR_back_rmse, MLR_manual_rmse, poisson_rmse, hurdle_rmse, 
           zip_rmse, neg_binom_rmse, neg_binom_alc_rmse)
)

print(comparison)
```

**Hurdle Poisson Regression gave us the lowest RMSE, meaning it outperformed the other models in accurately predicting the response variable, although the Multiple Linear Regression models came close in second.  We are selecting the Hurdle Poisson model given the RMSE, the least amount of predictors, and the intuitive sense the model makes.**

## Evaluation Data Set

**As our selected model only contains the predictors LabelAppeal, AcidIndex, and STARS, then the only transformation needed for the evaluation data set is to replace missing values in STARS with zero.**

### Zero Imputation for Variable STARS

```{r}
df_wine_eval_transformed  <- df_wine_eval %>% 
    mutate(STARS = replace(STARS, is.na(STARS) , 0))
```

### Predictions using the hurdle_poisson_model
```{r}
df_wine_eval_transformed$TARGET <- round( predict(hurdle_poisson_model, 
                                                  newdata = df_wine_eval_transformed, type = "response") )

table(df_wine_eval_transformed$TARGET)
```

