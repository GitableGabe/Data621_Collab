---
title: "DATA 621 Business Analytics and Data Mining"
date: "Last edited `r format(Sys.time(), '%B %d, %Y')`"
author: "Group 2 - Gabriel Campos, Melissa Bowman, Alexander Khaykin, & Jennifer Abinette"
output:
  pdf_document:
    latex_engine: xelatex
  geometry: "left=0.5cm,right=0.5cm,top=1cm,bottom=2cm"
  html_document:
    df_print: paged
---

## Homework #1 Assignment Requirements{.tabset}

### **Overview** 
In this homework assignment, you will explore, analyze and model a data set containing approximately 2200 records. Each record represents a professional baseball team from the years 1871 to 2006 inclusive. Each record has the performance of the team for the given year, with all of the statistics adjusted to match the performance of a 162 game season.

Your objective is to build a multiple linear regression model on the training data to predict the number of wins for the team. You can only use the variables given to you (or variables that you derive from the variables provided).

Below is a short description of the variables of interest in the data set:

<br>      

<table>


```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- "
| Variable Names         | Definition    					   	 | Theoretical Effect  	   |
|------------------------|:-------------------------------------:|------------------------:|
| INDEX 			     | Identification Variable (do not use)  | None 				   |
| TARGET_WINS 	         | Number of wins |      			     |  					   |
| TEAM_BATTING_H 	     | Base Hits by batters (1B,2B,3B,HR)    | Positive Impact on Wins |
| TEAM_BATTING_2B 	     | Doubles by batters (2B) 				 | Positive Impact on Wins |
| TEAM_BATTING_3B 	     | Triples by batters (3B) 				 | Positive Impact on Wins |
| TEAM_BATTING_HR 	     | Homeruns by batters (4B) 			 | Positive Impact on Wins |
| TEAM_BATTING_BB 	     | Walks by batters 					 | Positive Impact on Wins |
| TEAM_BATTING_HBP	     | Batters hit by pitch (get a free base)| Positive Impact on Wins |
| TEAM_BATTING_SO 	     | Strikeouts by batters 				 | Negative Impact on Wins |
| TEAM_BASERUN_SB 	     | Stolen bases 						 | Positive Impact on Wins |
| TEAM_BASERUN_CS 	     | Caught stealing 						 | Negative Impact on Wins |
| TEAM_FIELDING_E 	     | Errors 								 | Negative Impact on Wins |
| TEAM_FIELDING_DP	     | Double Plays 						 | Positive Impact on Wins |
| TEAM_PITCHING_BB	     | Walks allowed 						 | Negative Impact on Wins |
| TEAM_PITCHING_H 	     | Hits allowed 						 | Negative Impact on Wins |
| TEAM_PITCHING_HR	     | Homeruns allowed 					 | Negative Impact on Wins |
| TEAM_PITCHING_SO	     | Strikeouts by pitchers 			 	 | Positive Impact on Wins |
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```
### **Deliverable:**

* A write-up submitted in PDF format. Your write-up should have four sections. Each one is described below. You may assume you are addressing me as a fellow data scientist, so do not need to shy away from technical details.
* Assigned predictions (the number of wins for the team) for the evaluation data set.
* Include your R statistical programming code in an Appendix.

### **Write Up:**

1. **DATA EXPLORATION (25 Points)**
Describe the size and the variables in the moneyball training data set. Consider that too much detail will cause a manager to lose interest while too little detail will make the manager consider that you aren’t doing your job. Some suggestions are given below. Please do NOT treat this as a check list of things to do to complete the assignment.
You should have your own thoughts on what to tell the boss. These are just ideas.

a. Mean / Standard Deviation / Median
b. Bar Chart or Box Plot of the data
c. Is the data correlated to the target variable (or to other variables?)
d. Are any of the variables missing and need to be imputed “fixed”?

2. **DATA PREPARATION (25 Points)**
Describe how you have transformed the data by changing the original variables or creating new variables. If you did transform the data or create new variables, discuss why you did this. Here are some possible transformations.
a. Fix missing values (maybe with a Mean or Median value)
b. Create flags to suggest if a variable was missing
c. Transform data by putting it into buckets
d. Mathematical transforms such as log or square root (or use Box-Cox)
e. Combine variables (such as ratios or adding or multiplying) to create new variables

3. **BUILD MODELS (25 Points)**
Using the training data set, build at least three different multiple linear regression models, using different variables (or the same variables with different transformations). Since we have not yet covered automated variable selection methods, you should select the variables manually (unless you previously learned Forward or Stepwise selection, etc.). Since you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done.
Discuss the coefficients in the models, do they make sense? For example, if a team hits a lot of Home Runs, it would be reasonably expected that such a team would win more games. However, if the coefficient is negative
(suggesting that the team would lose more games), then that needs to be discussed. Are you keeping the model even though it is counter intuitive? Why? The boss needs to know.

4. **SELECT MODELS (25 Points)**
Decide on the criteria for selecting the best multiple linear regression model. Will you select a model with slightly worse performance if it makes more sense or is more parsimonious? Discuss why you selected your model.
For the multiple linear regression model, will you use a metric such as Adjusted R2
, RMSE, etc.? Be sure to explain how you can make inferences from the model, discuss multi-collinearity issues (if any), and discuss other relevant model output. Using the training data set, evaluate the multiple linear regression model based on (a) mean squared error, (b) R2, (c) F-statistic, and (d) residual plots. Make predictions using the evaluation data set.

\newpage

## **Evaluation** 

# 1. **DATA EXPLORATION**

```{r, echo=FALSE, message=FALSE, warnings=FALSE}
library(tidyr)
library(skimr)
library(dplyr)
library(stringr)
library(knitr)
library(ggplot2)
library(corrplot)
# Set so that long lines in R will be wrapped:
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)

# Added library for kdepairs
library(ResourceSelection)
```
## **Load the data**
```{r}
git_url<-
  "https://raw.githubusercontent.com/melbow2424/Data621_HW1/main/"

df_train <- 
  read.csv(paste0(git_url,"moneyball-training-data.csv"))

df_evaluation <- 
  read.csv(paste0(git_url,"moneyball-evaluation-data.csv"))
```

## **Summary of Variables**

```{r}
# Remove TEAM_ prefix from column names
df_train <-
  rename(df_train, 
         "BATTING_HITS"="TEAM_BATTING_H", "BATTING_2B"="TEAM_BATTING_2B",
         "BATTING_3B"="TEAM_BATTING_3B", "BATTING_HR"="TEAM_BATTING_HR",
         "BATTING_BB"="TEAM_BATTING_BB", "BASERUN_SB"="TEAM_BASERUN_SB",
         "BASERUN_CS"="TEAM_BASERUN_CS", "BATTING_HBP"="TEAM_BATTING_HBP",
         "PITCHING_HITS"="TEAM_PITCHING_H", "PITCHING_HR"="TEAM_PITCHING_HR",
         "PITCHING_BB"="TEAM_PITCHING_BB", "FIELD_ERRORS"="TEAM_FIELDING_E",
         "FIELD_DBLPLY"="TEAM_FIELDING_DP","BATTING_SO"="TEAM_BATTING_SO",
         "PITCHING_SO"="TEAM_PITCHING_SO")
df_evaluation <-
  rename(df_evaluation, 
         "BATTING_HITS"="TEAM_BATTING_H", "BATTING_2B"="TEAM_BATTING_2B",
         "BATTING_3B"="TEAM_BATTING_3B", "BATTING_HR"="TEAM_BATTING_HR", 
         "BATTING_BB"="TEAM_BATTING_BB", "BASERUN_SB"="TEAM_BASERUN_SB", 
         "BASERUN_CS"="TEAM_BASERUN_CS", "BATTING_HBP"="TEAM_BATTING_HBP",
         "PITCHING_HITS"="TEAM_PITCHING_H", "PITCHING_HR"="TEAM_PITCHING_HR", 
         "PITCHING_BB"="TEAM_PITCHING_BB",  "FIELD_ERRORS"="TEAM_FIELDING_E", 
         "FIELD_DBLPLY"="TEAM_FIELDING_DP","BATTING_SO"="TEAM_BATTING_SO",
         "PITCHING_SO"="TEAM_PITCHING_SO")

# Show variable stats for training dataset
print(skim(df_train))
```

### **Observations**
As shown above, the training data includes 17 variables (although Index is only for identification purposes) and 2,276 cases. We intend to create a Regression model that will predict the count of wins a team had over a 162 game season (TARGET_WINS) using the team's statistics on offensive and defensive plays. For these cases, teams had an average of 81 wins with a std deviation of 16 games with counts ranging from 0 to 146. The remaining variables include plays during offense, such as base hits and stealing bases (variables starting with BATTING or BASERUN), as well as defense plays, such as pitching strikeouts and double plays (PITCHING or FIELD).

Note that 6 variables are missing values including 2,085 values (92% missing) for batter hit by pitch (BATTING_HBP), 772 values (34%) for base runner caught stealing (BASERUN_CS), 286 values (13%) for fielding double plays (FIELD_DBLPLY), and the remaining three (stealing bases and strikeouts as batter and pitcher) missing less than 6% of values (BASERUN_SB, BATTING_SO, PITCHING_SO). We will explore how to handle these missing values later.

## **Distribution of Variables**

```{r}
df_train %>% 
  #pivot longer to plot all variables
  gather(variable, value, TARGET_WINS: FIELD_DBLPLY)%>% 
  ggplot(.,aes(x=value)) + #plotting every variable
  geom_density(fill = "darkgreen", alpha = 0.5) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()
```

### **Observations**
Histograms a good way to visualize the distributions of the original variables as shown above. We can see the response variable (TARGET_WINS) appears normally distributed. Several variables, such as BATTING_SO appear to be bimodal, which may resolve after the missing data is dealt with. Other variables like PITCHING_HITS appear to be skewed far left and may present a challenge unless imputation of missing values corrects this.

## **Correlation with Target Wins**

```{r}
cor(df_train, y=df_train$TARGET_WINS)

df_train %>%
  #pivot longer to plot all variables
  gather(variable, value, BATTING_HITS: FIELD_DBLPLY)%>% 
  ggplot(.,aes(x=value, y=TARGET_WINS)) + #plotting every variable
  geom_point(color = "darkgreen", alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()
```

### **Observations**
The plots above demonstrate the relationships between number of wins (TARGET_WINS) and each remaining variable (besides the variables missing values) that we will be exploring as predictors. The predictor variables with the strongest correlation to number of wins are total base hits (BATTING_HITS), doubles by batters (BATTING_2B) and walks by batters (BATTING_BB) which have correlations of .39, .29 and .23, respectively.

\newpage

# 2. **DATA PREPARATION**

## **Dealing with Missing Values - replace NA with mean, median, zero, or remove cases**
### **Mean Imputation**

```{r}
# Get the Means of columns in Data
train_means<-sapply(df_train, function(x) round(mean(x, na.rm = TRUE)))

# Replace NA values in 'column_name' with 'mean'
df_train_mn <- df_train %>%
  mutate(BATTING_SO =
           ifelse(is.na(BATTING_SO),
                  train_means[8],BATTING_SO))%>% 
  mutate(BASERUN_SB = 
           ifelse(is.na(BASERUN_SB),
                  train_means[9], BASERUN_SB))%>%
  mutate(BASERUN_CS =
           ifelse(is.na(BASERUN_CS),
                  train_means[10], BASERUN_CS))%>%
  mutate(BATTING_HBP = 
           ifelse(is.na(BATTING_HBP),
                  train_means[11],BATTING_HBP))%>%
  mutate(PITCHING_SO =
           ifelse(is.na(PITCHING_SO),
                  train_means[15], PITCHING_SO))%>%
  mutate(FIELD_DBLPLY =
           ifelse(is.na(FIELD_DBLPLY),
                  train_means[17], FIELD_DBLPLY))
```

```{r}
# Evaluate histograms
df_train_mn %>%
  #pivot longer to plot all variables
  gather(variable, value, TARGET_WINS: FIELD_DBLPLY)%>% 
  ggplot(.,aes(x=value)) + #plotting every variable
  geom_density(fill = "darkgreen", alpha = 0.5) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()
```

```{r}
# Fit a multiple linear regression model using lm with variables imputed:mean
model_mn <- lm(TARGET_WINS ~ BATTING_HITS+BATTING_2B+BATTING_3B+BATTING_HR+
                 BATTING_BB+BATTING_SO+BASERUN_SB+BASERUN_CS+BATTING_HBP+
                 PITCHING_HITS+PITCHING_HR+PITCHING_BB+PITCHING_SO+
                 FIELD_ERRORS+FIELD_DBLPLY, data = df_train_mn)

# Summary of the regression model
summary(model_mn)$adj.r.squared
```

#### **Observations**
After imputation with means:

* The response variable __TARGET_WINS__ still appears normally distributed
* The bimodality of the __BATTING__ variables is largely unresolved
* The far left skew of the __PITCHING__  variables is largely unresolved
* A Multiple Linear Regression with all variables has an adjusted R squared of .315

### **Median Imputation**


```{r}
# Get the Medians of columns in data
train_medians<-sapply(df_train, function(x) round(median(x, na.rm = TRUE)))

# Replace NA values in 'column_name' with 'median'
df_train_md <- df_train %>%
  mutate(BATTING_SO =
           ifelse(is.na(BATTING_SO),
                  train_medians[8],BATTING_SO))%>%
  mutate(BASERUN_SB =
           ifelse(is.na(BASERUN_SB),
                  train_medians[9], BASERUN_SB))%>%
  mutate(BASERUN_CS =
           ifelse(is.na(BASERUN_CS),
                  train_medians[10], BASERUN_CS))%>%
  mutate(BATTING_HBP =
           ifelse(is.na(BATTING_HBP),
                  train_medians[11],BATTING_HBP))%>%
  mutate(PITCHING_SO =
           ifelse(is.na(PITCHING_SO),
                  train_medians[15], PITCHING_SO))%>%
  mutate(FIELD_DBLPLY =
           ifelse(is.na(FIELD_DBLPLY),
                  train_medians[17], FIELD_DBLPLY))
```


```{r}
df_train_md %>% 
  #pivot longer to plot all variables
  gather(variable, value, TARGET_WINS: FIELD_DBLPLY)%>% 
  ggplot(.,aes(x=value)) + #plotting every variable
  geom_density(fill = "darkgreen", alpha = 0.5) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()
```
```{r}
# Fit a multiple linear regression model using lm with variables imputed:median
model_md <- lm(TARGET_WINS ~ BATTING_HITS+BATTING_2B+BATTING_3B+BATTING_HR+
                 BATTING_BB+BATTING_SO+BASERUN_SB+BASERUN_CS+BATTING_HBP+
                 PITCHING_HITS+PITCHING_HR+PITCHING_BB+PITCHING_SO+FIELD_ERRORS+
                 FIELD_DBLPLY, data = df_train_md)

# Summary of the regression model
summary(model_md)$adj.r.squared
```

#### **Observations**
After imputation with the median:

* The response variable __TARGET_WINS__ still appears normally distributed
* The bimodality of the __BATTING__ variables is largely unresolved
* The far left skew of the __PITCHING__  variables is largely unresolved  
* A Multiple Linear Regression with all variables has an adjusted R squared of .311

### **Zero Imputation**


```{r}
# Replace NA values with zero
df_train_0 <- df_train %>%
  replace_na( list( INDEX = 0,TARGET_WINS = 0,BATTING_HITS = 0,BATTING_2B = 0,
    BATTING_3B = 0,BATTING_HR = 0,BATTING_BB = 0,BATTING_SO = 0,BASERUN_SB = 0,
    BASERUN_CS = 0,BATTING_HBP = 0,PITCHING_HITS = 0,PITCHING_HR = 0,
    PITCHING_BB = 0,PITCHING_SO = 0,FIELD_ERRORS = 0,FIELD_DBLPLY = 0))
```


```{r}
df_train_0 %>% 
  #pivot longer to plot all variables
  gather(variable, value, TARGET_WINS: FIELD_DBLPLY)%>% 
  ggplot(.,aes(x=value)) + #plotting every variable
  geom_density(fill = "darkgreen", alpha = 0.5) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()
```
```{r}
# Fit a multiple linear regression model using lm with variables imputed:zero
model_0 <- lm(TARGET_WINS ~ BATTING_HITS+BATTING_2B+BATTING_3B+BATTING_HR+
                BATTING_BB+BATTING_SO+BASERUN_SB+BASERUN_CS+BATTING_HBP+
                PITCHING_HITS+PITCHING_HR+PITCHING_BB+PITCHING_SO+FIELD_ERRORS+
                FIELD_DBLPLY,data = df_train_0)

# Summary of the regression model
summary(model_0)$adj.r.squared
```

#### **Observations**
After imputation with the zero:

* Zero is a poor choice as it introduces strong peaks to the left of the distribution of many variables such as __FIELD_DBLPLY__ 
* A Multiple Linear Regression with all variables has an adjusted R squared of .294

### **Remove NA Values**


```{r}
# Remove all rows with NA
df_train_rm<- na.omit(df_train)

# Evaluate distributions
df_train_rm %>% 
  #pivot longer to plot all variables
  gather(variable, value, TARGET_WINS: FIELD_DBLPLY)%>% 
  ggplot(.,aes(x=value)) + #plotting every variable
  geom_density(fill = "darkgreen", alpha = 0.5) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()
```
```{r}
# Fit a multiple linear regression model using lm
model_rm <- lm(TARGET_WINS ~ BATTING_HITS+BATTING_2B+BATTING_3B+BATTING_HR+
                 BATTING_BB+BATTING_SO+BASERUN_SB+BASERUN_CS+BATTING_HBP+
                 PITCHING_HITS+PITCHING_HR+PITCHING_BB+PITCHING_SO+FIELD_ERRORS+
                 FIELD_DBLPLY,data = df_train_rm)

# Summary of the regression model
summary(model_rm)$adj.r.squared
```

```{r}
# How many values are missing from BATTING_HBP in the evaluation dataset
# that we will need to use our regression model to predict wins for?
print(sum(is.na(df_evaluation$BATTING_HBP)))
```

#### **Observations**
After Removal of missing data:

* Although removal of missing data resolves the distributions of many variables, the sample size is reduced to under 10% of the original dataset from 2,276 observations to only 191 observations in the training dataset
* Additionally, batters hit by pitches (BATTING_HBP) is missing values for 240 of 259 cases (93%) in the evaluation dataset which we will be using our regression model to predict wins for.
* Observed a more favorable adjusted R-squared value than imputation with mean, median or zero
* A Multiple Linear Regression with all variables has an adjusted R squared of .512


## **Transformations**
### **Square Root with Mean Imputation**


```{r}
df_train_sqrt <- sqrt(df_train_mn)

df_train_sqrt %>% 
  #pivot longer to plot all variables
  gather(variable, value, TARGET_WINS: FIELD_DBLPLY)%>% 
  ggplot(.,aes(x=value)) + #plotting every variable
  geom_density(fill = "red", alpha = 0.5) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()

```

#### **Observations**
Square root transformation is common to use when variables are counts to stabilize variance.  Given mean imputation offered the best Multiple Regression effect size of .315 without removing observations, we used that dataset to perform square root transformations on. We can see from the histograms above that this transformation has improved the distribution of the variables to be more normal.

### **Ratio with Zero Imputation**
Create dataset transforming variables to ratios. The Count of Wins will be divided by 162 to create a ratio of wins in the season.  Additionally, we calculated new variable Total Plays that is the total number of plays for each team. Total Plays sums the number of pitching hits, batter hits and the other Base runner and fielding variables.  Note that BATTING_HITS & PITCHING_HITS already include the counts for BATTING_2B, BATTING_3B, BATTING_HR and PITHCING_HR. The variables are then converted to a ratio using Total Plays as the denominator.

```{r}
df_train_ratio <- df_train_0 %>%
  mutate(Total_Plays = BATTING_HITS + BATTING_BB + BATTING_SO + BATTING_HBP + 
           BASERUN_SB + BASERUN_CS + 
           PITCHING_BB + PITCHING_HITS + PITCHING_SO + 
           FIELD_DBLPLY + FIELD_ERRORS) %>%
  ### Target_Wins is based on 162 game season
  mutate(TARGET_WINS_RATIO = TARGET_WINS/162) %>%  
  mutate(BATTING_HITS = BATTING_HITS/Total_Plays) %>%
  mutate(BATTING_2B = BATTING_2B/Total_Plays) %>%
  mutate(BATTING_3B = BATTING_3B/Total_Plays) %>%
  mutate(BATTING_HR = BATTING_HR/Total_Plays) %>%
  mutate(BATTING_BB = BATTING_BB/Total_Plays) %>%
  mutate(BATTING_SO = BATTING_SO/Total_Plays) %>%
  mutate(BATTING_HBP = BATTING_HBP/Total_Plays) %>%
  mutate(BASERUN_SB = BASERUN_SB/Total_Plays) %>%
  mutate(BASERUN_CS = BASERUN_CS/Total_Plays) %>%
  mutate(PITCHING_BB = PITCHING_BB/Total_Plays) %>%
  mutate(PITCHING_HITS = PITCHING_HITS/Total_Plays) %>%
  mutate(PITCHING_HR = PITCHING_HR/Total_Plays) %>%
  mutate(PITCHING_SO = PITCHING_SO/Total_Plays) %>%
  mutate(FIELD_DBLPLY = FIELD_DBLPLY/Total_Plays) %>%
  mutate(FIELD_ERRORS = FIELD_ERRORS/Total_Plays)
```

```{r}
df_train_ratio %>% 
  #pivot longer to plot all variables
  gather(variable, value, TARGET_WINS: TARGET_WINS_RATIO)%>%
  ggplot(.,aes(x=value)) + #plotting every variable
  geom_density(fill = "pink", alpha = 0.5) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()

```

#### **Observations**
Ratio transformation can also be used to stabilize variance and offers a model based on the total number of plays for each observation allowing us to use zero imputation for missing values.  We can see from the histograms above that this transformation has improved the distribution of the variables to be more normal compared to the original variables, but there are still skewed variables from zero imputation.

\newpage

# 3. **BUILD MODELS**
## Initial Model - No Changes to Variables -Adj.R .512

```{r}
model_initial <- lm(TARGET_WINS ~ BATTING_HITS+BATTING_2B +BATTING_3B+
                      BATTING_HR+BATTING_BB+BATTING_SO+BASERUN_SB+ BASERUN_CS+
                      BATTING_HBP +PITCHING_HITS+ PITCHING_HR+PITCHING_BB+
                      PITCHING_SO+FIELD_ERRORS+ FIELD_DBLPLY, data = df_train)

summary(model_initial)
```

#### **Observations**

* This model is significant overall (p < 2.2e-16) with 175 degrees of freedom
* Adjusted R-squared of .512
* 2 of 15 variable coefficients and the intercept coefficient are significant (p < .05)
* 2085 observations deleted due to missing values

**Let us explore three different models including:**

* A - Combining Variables due to Multicollinearity
* B - Backwards Selection and Intuitive Variable Coefficients
* C - Ratio of Wins in 162 Game Season

## A - **Combining Variables due to Multicollinearity**
This model starts with the data that uses the Mean Imputation and focuses on multicollinearity between variables**

### Correlations between Variables 


```{r}
df_train_mn %>% 
  select(-INDEX) %>% 
  cor(.,) %>% 
  corrplot(.,method = "ellipse", type = "lower", diag = FALSE)
```

#### **Observations**
* The correlogram confirms that none of the variable are very strongly correlated with __TARGET_WINS__, with the exception of __BATTING_HITS__ which has a modest positive correlation with wins
* Strong multicollinearity is seen between the following variable which needs to be taken into consideration when constructing the models: __FIELD_ERRORS__, __PITCHING_HR__, __BATTING_3B__, __BATTING_HR__
* It may also not be advisable to include __BATTING_HBP__ in the model because it has no correlation with wins or any other variable in the dataset

### A1 - **ALL Variables -Adj.R .315**

```{r}
summary(model_mn)
```

#### **Observations**
* Model overall is significant (p < 2.2e-16) with 2260 degrees of freedom
* Adjusted R-squared of .315  
* 9 of 15 variable coefficients and the intercept coefficient are significant (p < .05)
* 4 Variable coefficients have counter intuitive values: 
Positive impact on Wins, but Negative coefficient: BATTING_2B & Negative impact on Wins, but Positive coefficient: PITCHING_HR, PITCHING_BB, FIELD_DBLPLY

### A2 - **Removed BATTING_HBP for NA Values -Adj.R .315**
* Let's test the model removing batters hit by pitch __BATTING_HBP__ as more than 90% of values were missing and that the correlation matrix after mean imputation show no correlation with wins or any other variable.


```{r}
model_mn_2 <- lm(TARGET_WINS ~ BATTING_HITS+BATTING_2B+BATTING_BB+BATTING_SO+
                 BASERUN_SB+BASERUN_CS+FIELD_ERRORS+PITCHING_HR+BATTING_3B+
                   BATTING_HR ## +BATTING_HBP
               +PITCHING_HITS+PITCHING_BB+PITCHING_SO+FIELD_DBLPLY,
               data = df_train_mn)

# Summary of the regression model
summary(model_mn_2)
```

#### **Observations**
* Model overall is significant (p < 2.2e-16) with 2261 degrees of freedom
* Adjusted R-squared is unchanged at .315  
* 9 of 14 variable coefficients and the intercept coefficient are significant (p < .05)
* 5 Variable coefficients have counter intuitive values: 
Positive impact on Wins, but Negative coefficient: BATTING_2B & Negative impact on Wins, but Positive coefficient: PITCHING_HR, PITCHING_HITS, PITCHING_BB, FIELD_DBLPLY


### A3 - **Combining Variables with Multicollinearity -Adj.R .243**
* When examining the correlations, we observed some multicollinearity among the following variables:

BATTING_HITS and PITCHING_HITS
BATTING_HR and PITCHING_HR
BATTING_BB and PITCHING_BB
BATTING_SO and PITCHING_SO

* Due to the high correlation between these variables, we made the decision to combine them into single variables through addition.

```{r}
# Creating new data set with combined correlated variables and removed 
# correlated variables 
df_train_with_combo <- df_train_mn %>%
  mutate(team_H = BATTING_HITS + PITCHING_HITS,
         team_HR = BATTING_HR + PITCHING_HR,
         team_BB = BATTING_BB + PITCHING_BB,
         team_SO = BATTING_SO + PITCHING_SO)%>%
  select(-BATTING_HITS, -PITCHING_HITS, -BATTING_HR, -PITCHING_HR, -BATTING_BB,
         -PITCHING_BB,-BATTING_SO,-BATTING_HBP,- PITCHING_SO)

# Testing this new data set
model_mn_3_combo <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B
                  + BASERUN_SB + BASERUN_CS + FIELD_ERRORS + FIELD_DBLPLY
                  + team_H + team_HR + team_BB + team_SO,
                  data = df_train_with_combo)

# Summary of the regression model
summary(model_mn_3_combo)
```

#### **Observations**
* Model overall is significant (p < 2.2e-16) with 2265 degrees of freedom
* Adjusted R-squared decreased to .243 (A1 & A2 -> .315)  
* 9 of 10 variable coefficients and the intercept coefficient are significant (p < .01)
* 1 Variable coefficients has counter intuitive values: 
Positive impact on Wins, but Negative coefficient: FIELD_DBLPLY & Negative impact on Wins, but Positive coefficient: NONE
* Counter intuitive to create variables that add batting and pitching stats that should have an opposite impact on Wins


### A4 - **Remove BASERUN_CS for Non-significance -Adj.R .243**

```{r}
# Testing this new data set
model_mn_4_combo <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B
                  + BASERUN_SB + FIELD_ERRORS + FIELD_DBLPLY
                  + team_H + team_HR + team_BB + team_SO,
                  data = df_train_with_combo)

# Summary of the regression model
summary(model_mn_4_combo)
```

#### **Observations**
* Model overall is significant (p < 2.2e-16) with 2265 degrees of freedom
* Adjusted R-squared unchanged from previous model at .243 (A1 & A2 -> .315, A3 -> .243)  
* 9 of 9 variable coefficients and the intercept coefficient are significant (p < .01)
* 1 Variable coefficients has counter intuitive values: 
Positive impact on Wins, but Negative coefficient: FIELD_DBLPLY & Negative impact on Wins, but Positive coefficient: NONE
* Counter intuitive to create variables that add batting and pitching stats that should have an opposite impact on Wins

### A5 - **Remove BATTING_HBP after Manual Review -Adj.R .221**
* Let's redo the correlations to select the most highly correlated variables.

```{r}
# Create a subset that includes all columns
# all_columns_subset <- df_train_with_combo[, ]
# kdepairs(all_columns_subset)
```

```{r}
model_mn_5_combo <- lm(TARGET_WINS ~ team_H + team_BB + FIELD_ERRORS +
                         FIELD_DBLPLY+ team_SO+ team_HR ##+ BATTING_HBP
            + BATTING_3B +BASERUN_SB, data = df_train_with_combo)

summary(model_mn_5_combo)
```

#### **Observations**
* Model overall is significant (p < 2.2e-16) with 2267 degrees of freedom
* Adjusted R-squared decreased to .221 (A1 & A2 -> .315, A3 & A4 -> .243)  
* 8 of 8 variable coefficients and the intercept coefficient are significant (p < .01)
* 1 Variable coefficients has counter intuitive values: 
Positive impact on Wins, but Negative coefficient: FIELD_DBLPLY & Negative impact on Wins, but Positive coefficient: NONE
* Counter intuitive to create variables that add batting and pitching stats that should have an opposite impact on Wins

### **Best from A: Model A2 model_mn_2 -Adj.R .315**

The second version of the model (model_mn_2) has the highest effect size accounting for 31.5% of the variance in the Wins (TARGET_WINS).  Additionally, the overall model and 9 of 14 coefficients are significant, though 5 counter intuitive coefficients are included and there is multicollinearity between predictor variables.

----------------------------------------------------------------------

## B - **Backwards Selection and Intuitive Variable Coefficients**
This model starts with a dataset that used Mean Imputation & Square Root Transformed Variables and then changes the model based on the variable coefficients.

### B1 - **ALL Variables -Adj.R .355**


```{r}
# Fit a multiple linear regression model using lm with square root 
#transformed variables
model_sqrt_1 <- lm(TARGET_WINS ~ BATTING_HITS+BATTING_2B+BATTING_3B+
                     BATTING_HR+BATTING_BB+BATTING_SO+BASERUN_SB+
                 BASERUN_CS+BATTING_HBP+PITCHING_HITS+PITCHING_HR+
                   PITCHING_BB+PITCHING_SO+FIELD_ERRORS+FIELD_DBLPLY,
               data = df_train_sqrt)

# Summary of the regression model
summary(model_sqrt_1)
```

#### **Observations**
* Model overall is significant (p < 2.2e-16) with 2260 degrees of freedom
* Adjusted R-squared of .355 which is an improvement from the mean imputation models tested which are .315 or less.
* 10 of 15 variable coefficients and the intercept coefficient are significant (p < .05)
* 5 Variable coefficients have counter intuitive values: 
Positive impact on Wins, but Negative coefficient: BATTING_2B, BATTING_BB, FIELD_DBLPLY & Negative impact on Wins, but Positive coefficient: PITCHING_BB, PITCHING_HR


### B2 - **Removed BASERUN_CS and BATTING_HBP for NA Values -Adj.R .328**
* Let's explore a model excluding the two variables with the most missing values including base runner caught stealing and batters hit by pitch

```{r}
model_sqrt_2 <- lm(TARGET_WINS ~ BATTING_HITS+BATTING_2B+BATTING_3B+BATTING_HR+
                     BATTING_BB+BATTING_SO+BASERUN_SB+##BASERUN_CS + BATTING_HBP
                 +PITCHING_HITS+PITCHING_HR+PITCHING_BB+PITCHING_SO+FIELD_ERRORS,
               data = df_train_sqrt)
# Summary of the regression model
summary(model_sqrt_2)
```

#### **Observations**
* Model overall is significant (p < 2.2e-16) with 2263 degrees of freedom
* Adjusted R-squared decreased from .355 to .328  
* 8 of 12 variable coefficients and the intercept coefficient are significant (p < .05)
* 5 Variable coefficients have counter intuitive values: 
Positive impact on Wins, but Negative coefficient: BATTING_2B, BATTING_BB, BATTING_HR & Negative impact on Wins, but Positive coefficient: PITCHING_BB, PITCHING_HR


### B3 - **Removed BASERUN_CS, BATTING_HBP, BATTING_HR, BATTING_BB, PITCHING_HR & PITCHING_BB for Non-significance -Adj.R .320**
* Let's adjust this model to remove an additional 4 predictor variables that have non-significant coefficients including Batter & Pitcher home runs & walks

```{r}
model_sqrt_3 <- lm(TARGET_WINS ~ BATTING_HITS+BATTING_2B+BATTING_3B+
                  BATTING_SO+BASERUN_SB+##BASERUN_CS + BATTING_HBP
           +PITCHING_HITS##+ BATTING_HR + BATTING_BB + PITCHING_HR + PITCHING_BB
                 +PITCHING_SO+FIELD_ERRORS,
               data = df_train_sqrt)
# Summary of the regression model
summary(model_sqrt_3)
```

#### **Observations**

* Model overall is significant (p < 2.2e-16) with 2267 degrees of freedom
* Adjusted R-squared decreased slightly to .320 (B1 -> .355, B2 -> .328)
* 8 of 8 variable coefficients are significant (p < .01), though the intercept coefficient is not (p = .92)
* 1 Variable coefficient has counter intuitive values: 
Positive impact on Wins, but Negative coefficient: BATTING_2B & Negative impact on Wins, but Positive coefficient: NONE


### B4 - **Added BATTING_HBP to increase effect size -Adj.R .320**
* Let's try adding back in batters hit by pitch (BATTING_HBP) to see if that will increase our effect size back to when all variables were included.

```{r}
model_sqrt_4 <- lm(TARGET_WINS ~ BATTING_HITS+BATTING_2B+BATTING_3B+BATTING_SO+
                     BASERUN_SB+ ##BASERUN_CS 
                 + BATTING_HBP+PITCHING_HITS
                 ##+ BATTING_HR + BATTING_BB + PITCHING_HR + PITCHING_BB
                 +PITCHING_SO+FIELD_ERRORS,
               data = df_train_sqrt)
# Summary of the regression model
summary(model_sqrt_4)
```

#### **Observations**

* Model overall is significant (p < 2.2e-16) with 2266 degrees of freedom
* Adjusted R-squared unchanged at .320 (B1 -> .355, B2 -> .328, B3 -> .320)
* 8 of 9 variable coefficients are significant (p < .01), and the intercept coefficient is still not  significant, but has improved (p = .280)
* 1 Variable coefficient has counter intuitive values: 
Positive impact on Wins, but Negative coefficient: BATTING_2B & Negative impact on Wins, but Positive coefficient: NONE


### **Best from B: Model B4 model_sqrt_4 -Adj.R .320**

The fourth version of the square root transformation model (model_sqrt_4) explains 32% of the variance in Wins.  Although this is 3.5% less variance than the first model, this fourth model ties more closely to what we would expect of the coefficient values and includes only significant predictor variables.

----------------------------------------------------------------------

## C - **Ratio of Wins in 162 Game Season**
This model starts with a dataset with Zero Imputation & Ratio Transformed Variables (df_train_ratio). The response variable (number of wins) has been converted to a ratio of wins out of a 162 game season. Additionally, the variable Total Plays was introduced that totals number of plays for each observation aka team. Total Plays sums the number of pitching hits, batter hits and the other Base runner and fielding variables. The remaining variables were converted to a ratio using Total Plays as the denominator.

### C1 - **ALL Ratio Variables -Adj.R .232**


```{r}
# Fit a multiple linear regression model
model_ratio_1 <- lm(TARGET_WINS_RATIO ~ BATTING_HITS+BATTING_2B+BATTING_3B+
                      BATTING_HR+BATTING_BB+BATTING_SO+BASERUN_SB+
                 BASERUN_CS+BATTING_HBP+PITCHING_HITS+PITCHING_HR+
                   PITCHING_BB+PITCHING_SO+FIELD_ERRORS+FIELD_DBLPLY,
               data = df_train_ratio)

# Summary of the regression model
summary(model_ratio_1)
```

#### **Observations**

* Model overall is significant (p < 2.2e-16) with 2261 degrees of freedom
* This model has an Adjusted R-squared of .232  
* 13 of 15 variable coefficients and the Intercept coefficient are significant (p < .05) 
* 7 Variable coefficients have counter intuitive values: 
Positive impact on Wins, but Negative coefficient:  BATTING_HR & Negative impact on Wins, but Positive coefficient: BATTING_SO, BASERUN_CS, PITCHING_HITS, PITCHING_BB, FIELD_ERRORS & No Coefficient: FIELD_DBLPLY

### C2 - **Removed FIELD_DBLPLY & BATTING_HR for Non-significance -Adj.R .232**
* Let us explore a model excluding the variables that do not have significant coefficients including field doubleplays, and batter home runs (FIELD_DBLPLY & BATTING_HR)

```{r}
# Fit a multiple linear regression model using lm
model_ratio_2 <- lm(TARGET_WINS_RATIO ~ BATTING_HITS+BATTING_2B+BATTING_3B
                    ## +BATTING_HR + FIELD_DBLPLY
                    +BATTING_BB+BATTING_SO+BASERUN_SB
                    +BASERUN_CS+BATTING_HBP+PITCHING_HITS+PITCHING_HR+
                      PITCHING_BB+PITCHING_SO+FIELD_ERRORS,
               data = df_train_ratio)

# Summary of the regression model
summary(model_ratio_2)
```

#### **Observations**
* Model overall is significant (p < 2.2e-16) with 2262 degrees of freedom
* Adjusted R-squared is unchanged at .232  
* 13 of 13 variable coefficients and the Intercept coefficient are significant (p < .05) 
* 4 Variable coefficients have counter intuitive values: 
Positive impact on Wins, but Negative coefficient: NONE & Negative impact on Wins, but Positive coefficient: BASERUN_CS, PITCHING_HITS, PITCHING_HR, PITCHING_BB


### C3 - **Removed FIELD_DBLPLY, BATTING_HR & BATTING_HBP for Least significance  -Adj.R .230**
* Let's adjust this model to remove an additional predictor variables that has the least significant variable coefficient batter hit by pitch (BATTING_HBP p = .01) and was also the variable missing over 90% of values.

```{r}
# Fit a multiple linear regression model using lm
model_ratio_3 <- lm(TARGET_WINS_RATIO ~ BATTING_HITS+BATTING_2B+BATTING_3B
                    ## +BATTING_HR + FIELD_DBLPLY + BATTING_HBP
                    +BATTING_BB+BATTING_SO+BASERUN_SB
                    +BASERUN_CS+PITCHING_HITS+PITCHING_HR+PITCHING_BB+
                      PITCHING_SO+FIELD_ERRORS,
               data = df_train_ratio)

# Summary of the regression model
summary(model_ratio_3)
```

#### **Observations**
* Model overall is significant (p < 2.2e-16) with 2263 degrees of freedom
* Adjusted R-squared decreased very slightly to .230 (C1 & C2 -> .232)
* 12 of 12 variable coefficients and the Intercept coefficient are significant (p < .001) 
* 5 Variable coefficients have counter intuitive values: 
Positive impact on Wins, but Negative coefficient: NONE & Negative impact on Wins, but Positive coefficient: BATTING_SO, BASERUN_CS, PITCHING_HITS, PITCHING_HR, PITCHING_BB

### C4 - **Removed BATTING_SO, BASERUN_CS, PITCHING_HITS, PITCHING_HR, PITCHING_BB for Counter intuitive coefficients -Adj.R .105**
* Let's adjust this model to remove additional predictor variables that should have a negative impact on Wins, but have a positive coefficient (Batting strikeouts, base runner caught stealing, and pitching hits, home runs & walks)

```{r}
# Fit a multiple linear regression model using lm
model_ratio_4 <- lm(TARGET_WINS_RATIO ~ BATTING_HITS+BATTING_2B+BATTING_3B
                    ## +BATTING_HR + FIELD_DBLPLY + BATTING_HBP
                    +BATTING_BB+BASERUN_SB
                ## +BATTING_SO+BASERUN_CS+PITCHING_HITS+PITCHING_HR+PITCHING_BB 
                    + PITCHING_SO +  FIELD_ERRORS,
               data = df_train_ratio)

# Summary of the regression model
summary(model_ratio_4)
```

#### **Observations**
* Model overall is significant (p < 2.2e-16) with 2268 degrees of freedom
* Adjusted R-squared decreased to .105 (C1 & C2 -> .232, C3 -> .230)
* 6 of 7 variable coefficients and the Intercept coefficient are significant (p < .05) 
* 4 Variable coefficients have counter intuitive values: 
Positive impact on Wins, but Negative coefficient: BATTING_HITS, BATTING_BB & Negative impact on Wins, but Positive coefficient: PITCHING_SO, FIELD_ERRORS


### **Best from C: Model C3 model_ratio_3 -Adj.R .230**

The third version of the model (model_ratio_3) has essentially the same effect size as the previous 2 and is .13 higher than the fourth model. It accounts for 23% of the variance in the ratio of wins (TARGET_WINS_RATIO) and the overall model and coefficients are all significant, though 5 counter intuitive coefficients are included.

\newpage

# 4. **SELECT MODELS**
## **Selection Criteria to Consider**
As all three models are significant overall with similar degrees of freedom, we will focus on:
* Adjusted R-squared value
* Significance of Variable Coefficients
* Variable Coefficients are Intuitive

Combining Variables due to Multicollinearity (Model A2):

* Adjusted R-squared of .315  
* 9 of 14 variable coefficients and the intercept coefficient are significant (p < .05)
* 5 Variable coefficients have counter intuitive values: 
Positive impact on Wins, but Negative coefficient: batter doubles (BATTING_2B) & Negative impact on Wins, but Positive coefficient: field double play and pitching hits, home runs, & walks (PITCHING_HR, PITCHING_HITS, PITCHING_BB, FIELD_DBLPLY)


Backwards Selection & Intuitive Variable Coefficient (Model B4):

* Adjusted R-squared of .320
* 8 of 9 variable coefficients are significant (p < .01) though intercept coefficient is not
* 1 Variable coefficient has counter intuitive values as we would expect batter doubles (BATTING_2B) to have a positive impact on Wins, but it has a negative coefficient.


Percent Wins in 162 Game Seasons (Model C3):

* Adjusted R-squared of .230
* 12 of 12 variable coefficients and the Intercept coefficient are significant (p < .001) 
* 5 Variable coefficients have counter intuitive values as we would predict a negative impact on Wins for batter strikeouts, base runners caught stealing, and pitching hits, home runs & walks (BATTING_SO, BASERUN_CS, PITCHING_HITS, PITCHING_HR, PITCHING_BB)

### **Selected Model**
We chose Model B4 that incorporated backwards selection and focusing on variable coefficients being intuitive. It has the greatest adjusted R-squared accounting for 32% of the variance in Wins. It is also the model that makes the most intuitive sense overall as it focused on maximizing intuitiveness of the model through variable coefficient values, which are also all significant barring one. 

### **Regression Summary for Selected Model

```{r}
summary(model_sqrt_4)
# Save coefficients from Multiple Regression
df_coeff <- as.data.frame(model_sqrt_4$coefficients) %>%
  t()
```

### **Calculate Mean Squared Error**

```{r}
model_summ <-summary(model_sqrt_4)
#calculate MSE
print(mean(model_summ$residuals^2))
```


### **Bivariate Plots of Wins by Significant Predictors**

```{r}
p1 <- model_sqrt_4 %>% 
  ggplot(aes(y=TARGET_WINS, x=BATTING_HITS)) +
  geom_point(color="darkgreen") +
  geom_smooth(method = "lm", se=TRUE) +
  labs(title = "WINS BY BATTING HITS",
       x="TOTAL HITS", y="WINS") +
  theme_bw()
p2 <- model_sqrt_4 %>% 
  ggplot(aes(y=TARGET_WINS, x=BATTING_2B)) +
  geom_point(color="darkgreen") +
  geom_smooth(method = "lm", se=TRUE) +
  labs(title = "WINS BY BATTING DOUBLES",
       x="DOUBLES HIT", y="WINS") +
  theme_bw()
p3 <- model_sqrt_4 %>% 
  ggplot(aes(y=TARGET_WINS, x=BATTING_3B)) +
  geom_point(color="darkgreen") +
  geom_smooth(method = "lm", se=TRUE) +
  labs(title = "WINS BY BATTING TRIPLES",
       x="TRIPLES HIT", y="WINS") +
  theme_bw()
p4 <- model_sqrt_4 %>% 
  ggplot(aes(y=TARGET_WINS, x=BATTING_SO)) +
  geom_point(color="darkgreen") +
  geom_smooth(method = "lm", se=TRUE) +
  labs(title = "WINS BY BATTING STRIKEOUTS",
       x="BATTER STRIKEOUTS", y="WINS") +
  theme_bw()
p5 <- model_sqrt_4 %>% 
  ggplot(aes(y=TARGET_WINS, x=BASERUN_SB)) +
  geom_point(color="darkgreen") +
  geom_smooth(method = "lm", se=TRUE) +
  labs(title = "WINS BY BASE RUNNER STOLEN BASES",
       x="STOLEN BASES", y="WINS") +
  theme_bw()
p6 <- model_sqrt_4 %>% 
  ggplot(aes(y=TARGET_WINS, x=PITCHING_HITS)) +
  geom_point(color="darkgreen") +
  geom_smooth(method = "lm", se=TRUE) +
  labs(title = "WINS BY HITS OFF PITCHER",
       x="TOTAL HITS", y="WINS") +
  theme_bw()
p7 <- model_sqrt_4 %>% 
  ggplot(aes(y=TARGET_WINS, x=PITCHING_SO)) +
  geom_point(color="darkgreen") +
  geom_smooth(method = "lm", se=TRUE) +
  labs(title = "WINS BY PITCHER STRIKEOUTS",
       x="PITCHER STRIEKOUTS", y="WINS") +
  theme_bw()
p8 <- model_sqrt_4 %>% 
  ggplot(aes(y=TARGET_WINS, x=FIELD_ERRORS)) +
  geom_point(color="darkgreen") +
  geom_smooth(method = "lm", se=TRUE) +
  labs(title = "WINS BY FIELDING ERRORS",
       x="FIELD ERRORS", y="WINS") +
  theme_bw()

library(gridExtra)
grid.arrange(p1, p2, p3, p4, p5, p6, p7,p8, nrow=2)
```

### **Diagnostic Plots for Selected Model**
```{r}
ggplot(data=model_sqrt_4, aes(model_sqrt_4$residuals)) +
  geom_histogram(binwidth = 1, color = "black", fill = "purple4") +
  theme(panel.background = element_rect(fill = "white"),
        axis.line.x=element_line(),
        axis.line.y=element_line()) +
  ggtitle("Histogram for Model Residuals")
```


```{r}
par(mfrow = c(2, 2))
plot(model_sqrt_4)
```

### **Observations**
A - Mean squared Error: 0.5866497
B - Adjusted R-squared is .320 indicating the model accounts for 32% of the variability in our training dataset
C - F-statistic: 120.2 on 9 and 2266 DF,  p-value: < 2.2e-16
D - Residual Plots: 
* Nearly Normal Residuals - Condition is met based on the histogram and normal probability plots though the 2 ends diverge on the Q-Q plot
* Linearity and Constant Variability - There is no apparent pattern in the residuals plot indicating there is linearity and the points are scattered around zero for the most part showing constant variability
* Leverage points - There are several bad leverage points affecting our model as indicated by Cook's distance


## **Predict Total Wins for Evaluation Dataset**
### **Summary of Variables**

```{r}
print(skim(df_evaluation))
```

### **Mean Imputation then Square Root Transformation**

```{r}
# Get the Means of columns in Data
evaluation_means<-sapply(df_evaluation, function(x) round(mean(x, na.rm =TRUE)))

# Replace NA values in 'column_name' with 'mean'
df_evaluation_mn <- df_evaluation %>%
  mutate(BATTING_SO =
           ifelse(is.na(BATTING_SO),
                  evaluation_means[8],BATTING_SO))%>% 
  mutate(BASERUN_SB = 
           ifelse(is.na(BASERUN_SB),
                  evaluation_means[9], BASERUN_SB))%>%
  mutate(BASERUN_CS =
           ifelse(is.na(BASERUN_CS),
                  evaluation_means[10], BASERUN_CS))%>%
  mutate(BATTING_HBP = 
           ifelse(is.na(BATTING_HBP),
                  evaluation_means[11],BATTING_HBP))%>%
  mutate(PITCHING_SO =
           ifelse(is.na(PITCHING_SO),
                  evaluation_means[15], PITCHING_SO))%>%
  mutate(FIELD_DBLPLY =
           ifelse(is.na(FIELD_DBLPLY),
                  evaluation_means[17], FIELD_DBLPLY))

df_evaluation_sqrt <- sqrt(df_evaluation_mn)

df_evaluation_sqrt %>%
  #pivot longer to plot all variables
  gather(variable, value, BATTING_HITS: FIELD_DBLPLY)%>% 
  ggplot(.,aes(x=value)) + #plotting every variable
  geom_density(fill = "purple", alpha = 0.5) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()
```

### **Predict Total Wins**
* Use Regression Model built using Training data to Predict Wins for the Evaluation data
```{r}
df_evaluation_sqrt$PREDICT_WINS_sqrt = 
  predict(model_sqrt_4, new = df_evaluation_sqrt)
df_evaluation_sqrt$PREDICT_WINS = 
  (df_evaluation_sqrt$PREDICT_WINS_sqrt)*(df_evaluation_sqrt$PREDICT_WINS_sqrt)
```


### Reference

* "Pythagorean Theorem of Baseball." Baseball Reference, https://www.baseball-reference.com/bullpen/Pythagorean_Theorem_of_Baseball. Accessed 11 September 2023.
* No author listed. "Pythagorean Expectation in Major League Baseball." Digital Commons @ Cal Poly, https://digitalcommons.calpoly.edu/cgi/viewcontent.cgi?article=1067&context=statsp. Accessed 11 September 2023.