---
title: "DATA 621: BUSINESS ANALYTICS AND DATA MINING HOMEWORK#4: LOGISTIC REGRESSION"
date: "Last edited `r format(Sys.time(), '%B %d, %Y')`"
author: "Group 2 - Gabriel Campos, Melissa Bowman, Alexander Khaykin, & Jennifer Abinette"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 4
    number_sections: true
    highlight: tango
  geometry: "left=0.5cm,right=0.5cm,top=0.5cm,bottom=0.5cm"
  html_document:
    df_print: paged
urlcolor: blue
---

*Overview*

&emsp; In this homework assignment, you will explore, analyze and model a data set containing approximately 8000
records representing a customer at an auto insurance company. Each record has two response variables. The
first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero
means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero
if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.

&emsp; Your objective is to build multiple linear regression and binary logistic regression models on the training data
to predict the probability that a person will crash their car and also the amount of money it will cost if the person
does crash their car. You can only use the variables given to you (or variables that you derive from the variables
provided). Below is a short description of the variables of interest in the data set:
<br>

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)

library(tidyverse)
library(ggplot2)
library(Hmisc)
library(corrplot)
library(psych)
library(car)
library(caret)
options(scipen = 999)
```

\

Deliverables

* A write-up submitted in PDF format. Your write-up should have four sections. Each one is described below. You may assume you are addressing me as a fellow data scientist, so do not need to shy away from technical details.
* Assigned predictions (probabilities, classifications, cost) for the evaluation data set. Use 0.5 threshold.
* Include your R statistical programming code in an Appendix.

Write Up:

1. DATA EXPLORATION (25 Points)

Describe the size and the variables in the insurance training data set. Consider that too much detail will cause a manager to lose interest while too little detail will make the manager consider that you aren’t doing your job. Some suggestions are given below. Please do NOT treat this as a check list of things to do to complete the assignment. You should have your own thoughts on what to tell the boss. These are just ideas.

a. Mean / Standard Deviation / Median
b. Bar Chart or Box Plot of the data
c. Is the data correlated to the target variable (or to other variables?)
d. Are any of the variables missing and need to be imputed “fixed”?

2. DATA PREPARATION (25 Points)

Describe how you have transformed the data by changing the original variables or creating new variables. If you did transform the data or create new variables, discuss why you did this. Here are some possible transformations.

a. Fix missing values (maybe with a Mean or Median value)
b. Create flags to suggest if a variable was missing
c. Transform data by putting it into buckets
d. Mathematical transforms such as log or square root (or use Box-Cox)
e. Combine variables (such as ratios or adding or multiplying) to create new variables

3. BUILD MODELS (25 Points)

Using the training data set, build at least two different multiple linear regression models and three different binary logistic regression models, using different variables (or the same variables with different transformations). You may select the variables manually, use an approach such as Forward or Stepwise, use a different approach such as trees, or use a combination of techniques. Describe the techniques you used. If you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done. 


Discuss the coefficients in the models, do they make sense? For example, if a person has a lot of traffic tickets, you would reasonably expect that person to have more car crashes. If the coefficient is negative (suggesting that the person is a safer driver), then that needs to be discussed. Are you keeping the model even though it is counter intuitive? Why? The boss needs to know.


4. SELECT MODELS (25 Points)

Decide on the criteria for selecting the best multiple linear regression model and the best binary logistic regression model. Will you select models with slightly worse performance if it makes more sense or is more parsimonious? Discuss why you selected your models.


For the multiple linear regression model, will you use a metric such as Adjusted R2, RMSE, etc.? Be sure to explain how you can make inferences from the model, discuss multi-collinearity issues (if any), and discuss other relevant model output. Using the training data set, evaluate the multiple linear regression model based on (a) mean squared error, (b) R2 , (c) F-statistic, and (d) residual plots. For the binary logistic regression model, will you use a metric such as log likelihood, AIC, ROC curve, etc.? Using the training data set, evaluate the binary logistic regression model based on (a) accuracy, (b) classification error rate, (c) precision, (d) sensitivity, (e) specificity, (f) F1 score, (g) AUC, and (h) confusion matrix. Make predictions using the evaluation data set.

# **DATA EXPLORATION & PREPARATION**

## Import Data

### Training Dataset

```{r, echo=FALSE}
url_git <- "https://raw.githubusercontent.com/GitableGabe/Data621_Data/main/"
```

```{r}
df_insur_train <- 
  read.csv(paste0(url_git,"insurance_training_data.csv"))

head(df_insur_train)
```

```{r}
dim(df_insur_train)
```

**In the training dataset, there are 8,161 rows and 26 columns. We will remove the INDEX column because it is a unique identifier and will not be used.The two outcome variables are:**

* TARGET_FLAG - a 0/1 variable that indicates if a insurance client has been in a car accident

* TARGET_AMT - a numeric variable that of insurance claim payout per car accident 

```{r}
df_insur_train <- df_insur_train %>% 
  select(-INDEX)
```

### Evaluation Dataset

```{r}
df_insur_eval <- 
  read.csv(paste0(url_git,"insurance-evaluation-data.csv"))

head(df_insur_eval)
```

```{r}
df_insur_eval <- df_insur_eval %>% 
  select(-INDEX)
```


* There are 12 variables with discrete values and 13 variables with continuous values

## Transformations

* We noticed that there are characters in several of the columns that need to be cleaned up before the analysis. These will be removed and if necessary the variable will be converted to the appropriate data type.

```{r}
df_insur_train <- df_insur_train %>% 
  mutate(INCOME = gsub("\\$", "", INCOME), HOME_VAL = gsub("\\$", "", HOME_VAL),
         BLUEBOOK = gsub("\\$", "", BLUEBOOK), OLDCLAIM = gsub("\\$", "",
                                                               OLDCLAIM)) %>% 
  mutate(INCOME = gsub(",", "", INCOME), HOME_VAL = gsub(",", "", HOME_VAL),
         BLUEBOOK = gsub(",", "", BLUEBOOK), OLDCLAIM = gsub(",", "",
                                                             OLDCLAIM)) %>% 
  mutate(INCOME = as.numeric(INCOME), HOME_VAL = as.numeric(HOME_VAL),
         BLUEBOOK = as.numeric(BLUEBOOK), OLDCLAIM = as.numeric(OLDCLAIM))
```

```{r}
df_insur_train <- df_insur_train %>% 
  mutate(MSTATUS = gsub("z_","", MSTATUS), SEX = gsub("z_","", SEX),
         EDUCATION = gsub("z_","", EDUCATION), JOB = gsub("z_","", JOB),
         CAR_TYPE = gsub("z_","", CAR_TYPE), URBANICITY = gsub("z_","",
                                                               URBANICITY))
```

* Applied same to evaluation data

```{r}
df_insur_eval <- df_insur_eval %>% 
  mutate(INCOME = gsub("\\$", "", INCOME), HOME_VAL = gsub("\\$", "", HOME_VAL),
         BLUEBOOK = gsub("\\$", "", BLUEBOOK), OLDCLAIM = gsub("\\$", "",
                                                               OLDCLAIM)) %>% 
  mutate(INCOME = gsub(",", "", INCOME), HOME_VAL = gsub(",", "", HOME_VAL),
         BLUEBOOK = gsub(",", "", BLUEBOOK), OLDCLAIM = gsub(",", "",
                                                             OLDCLAIM)) %>% 
  mutate(INCOME = as.numeric(INCOME), HOME_VAL = as.numeric(HOME_VAL),
         BLUEBOOK = as.numeric(BLUEBOOK), OLDCLAIM = as.numeric(OLDCLAIM))
```

```{r}
df_insur_eval <- df_insur_eval %>% 
  mutate(MSTATUS = gsub("z_","", MSTATUS), SEX = gsub("z_","", SEX),
         EDUCATION = gsub("z_","", EDUCATION), JOB = gsub("z_","", JOB),
         CAR_TYPE = gsub("z_","", CAR_TYPE), URBANICITY = gsub("z_","",
                                                               URBANICITY))
```

* We will recode JOB into White Collar(Clerical, Doctor, Lawyer, Manager, and Professional), Blue Collar, and None (Student, Homemaker)
```{r}
df_insur_train <- df_insur_train %>% 
  mutate(JOB = ifelse(JOB=="Blue Collar", "Blue Collar", 
                      ifelse(JOB=="Student" | JOB=="Home Maker",
                             "None",
                             "White Collar")))
df_insur_eval <- df_insur_eval %>% 
  mutate(JOB = ifelse(JOB=="Blue Collar", "Blue Collar", 
                      ifelse(JOB=="Student" | JOB=="Home Maker",
                             "None", "White Collar")))
```

* We will also recode KIDSDRIV into a 0 or 1 (1+kids driving). Because there are a lot more insurance claims without kids dring than with kids driving.
```{r}
df_insur_train <- df_insur_train %>% 
  mutate(KIDSDRIV = ifelse(KIDSDRIV >= 1, 1, 0))

df_insur_eval <- df_insur_eval %>% 
  mutate(KIDSDRIV = ifelse(KIDSDRIV >= 1, 1, 0))
```

* Also, recode the yes/mo labels for marital status, parent status, red car, and revoked license variables as 1/0.

```{r}
df_insur_train <- df_insur_train %>% 
  mutate(MSTATUS = ifelse(MSTATUS == "No", "0", "1"),
         PARENT1 = ifelse(PARENT1 == "No", "0", "1"),
         RED_CAR = ifelse(RED_CAR == "no", "0", "1"),
         REVOKED = ifelse(REVOKED == "No", "0", "1"))
df_insur_eval <- df_insur_eval %>% 
  mutate(MSTATUS = ifelse(MSTATUS == "No", "0", "1"),
         PARENT1 = ifelse(PARENT1 == "No", "0", "1"),
         RED_CAR = ifelse(RED_CAR == "no", "0", "1"),
         REVOKED = ifelse(REVOKED == "No", "0", "1"))
```

* Lastly we will shorten the lables for Urbanicity and Turn Education into a factor with "< Highschool" as the reference variable.

```{r}
df_insur_train <- df_insur_train %>% 
  mutate(URBANICITY = ifelse(URBANICITY == "Highly Urban/ Urban",
                             "Urban", "Rural")) %>% 
  mutate(EDUCATION = factor(EDUCATION,levels = c("<High School",
                                                 "High School",
                                                 "Bachelors",
                                                 "Masters",
                                                 "PhD")))

df_insur_eval <- df_insur_eval %>% 
  mutate(URBANICITY = ifelse(URBANICITY == "Highly Urban/ Urban",
                             "Urban", "Rural")) %>% 
  mutate(EDUCATION = factor(EDUCATION,levels = c("<High School",
                                                 "High School",
                                                 "Bachelors",
                                                 "Masters",
                                                 "PhD")))
```

## Missing Data Imputation

### Training Dataset

```{r}
#loop to count the NAs for each column
for (i in colnames(df_insur_train)){
  print(paste(i,"  ", sum(is.na(df_insur_train[,i])),sep = ""))
}
```

* There are NAs in three variable columns, 6 in AGE, 454 in YOJ (Years on the job) , and 510 in CAR_AGE. For these variable we will impute the median so as not to create an over fitting problem. Also, there was an irrational value of negative 3 for CAR_AGE, we replaced it with zero.

```{r}
df_insur_train <- df_insur_train %>% 
  mutate(AGE = ifelse(is.na(AGE),
                      median(AGE, na.rm = TRUE),
                      AGE), YOJ = ifelse(is.na(YOJ),
                                         median(YOJ, na.rm = TRUE), YOJ),
         CAR_AGE = ifelse(is.na(CAR_AGE),
                          median(CAR_AGE, na.rm = TRUE), CAR_AGE), 
         HOME_VAL = ifelse(is.na(HOME_VAL),
                           median(HOME_VAL,
                                  na.rm = TRUE), HOME_VAL),
         INCOME = ifelse(is.na(INCOME),
                         median(INCOME, na.rm = TRUE),
                         INCOME)) %>%
  mutate(CAR_AGE = ifelse(CAR_AGE < 0, 0, CAR_AGE))

summary(df_insur_train$CAR_AGE)
```

### Evaluation Dataset

```{r}
#loop to count the NAs for each column
for (i in colnames(df_insur_eval)){
  print(paste(i,"  ", sum(is.na(df_insur_eval[,i])),sep = ""))
}
```

* There are NAs in five variable columns, 1 in AGE, 94 in YOJ (Years on the job) , 125 in INCOME, 111 HOME_VAL, and 129 in CAR_AGE. For these variable we will impute the median so as not to create an over fitting problem. 

```{r}
df_insur_eval <- df_insur_eval %>% 
  mutate(AGE = ifelse(is.na(AGE), median(AGE, na.rm = TRUE),
                      AGE), YOJ = ifelse(is.na(YOJ),
                                         median(YOJ, na.rm = TRUE), YOJ),
         CAR_AGE = ifelse(is.na(CAR_AGE), median(CAR_AGE, na.rm = TRUE),
                          CAR_AGE)) %>% 
  mutate(INCOME = ifelse(is.na(INCOME), median(INCOME,
                                               na.rm = TRUE), INCOME),
         HOME_VAL = ifelse(is.na(HOME_VAL),
                           median(HOME_VAL, na.rm = TRUE), YOJ))

summary(df_insur_eval$CAR_AGE)
```


## Exploratory Data Analysis

**Summary statistics for the numeric variables:**

```{r}
df_insur_train %>% 
  select(TARGET_AMT, AGE, YOJ, INCOME, HOME_VAL, TRAVTIME, BLUEBOOK, TIF,
         OLDCLAIM, CLM_FREQ, MVR_PTS, CAR_AGE, HOMEKIDS) %>% 
  describe()
```

* The skewness and Kurtosis values for the outcome variable TARGET_AMT strongly suggests that the distribution is likely not normal.

```{r}
df_insur_train %>% 
  select(TARGET_AMT, AGE, YOJ, INCOME, HOME_VAL, TRAVTIME, BLUEBOOK,
         TIF, OLDCLAIM, CLM_FREQ, MVR_PTS, CAR_AGE, HOMEKIDS) %>% 
  gather() %>% 
  ggplot(aes(x = value)) + 
  geom_histogram(fill = "cadetblue") + 
  facet_wrap(~key, scales = "free")
```

* The histogram for TARGET_AMT, CAR_AGE, CLM_FREQ,HOME_VAL, INCOME, MRV_PTS, OLDCLAIM, and TIF are clearly not normally distributed and will need to be transformed if the residuals are not normally distributed.

* We will explore the proportions of the discrete variables.

```{r}
df_insur_train %>% 
  select(TARGET_FLAG, KIDSDRIV, PARENT1, MSTATUS, SEX, EDUCATION,
         JOB, CAR_USE, CAR_TYPE, RED_CAR, REVOKED, URBANICITY) %>% 
  gather() %>% 
  ggplot(aes(x = value)) + 
  geom_bar(fill = "cadetblue") + 
  coord_flip()+
  facet_wrap(~key, scales = "free")
```

* To check for collinearity through the correlation of the variables

```{r}
mat <- df_insur_train %>% 
  select(-CAR_TYPE, -CAR_USE, -EDUCATION, -JOB, -SEX, -URBANICITY) %>% 
  mutate(PARENT1 = as.numeric(PARENT1), MSTATUS = as.numeric(MSTATUS),
         RED_CAR = as.numeric(RED_CAR), REVOKED = as.numeric(REVOKED)) %>% 
  cor()
corrplot(mat, method = "circle", diag = FALSE, order ="hclust", type = "lower")
```

* We do not seem to have very much concern for high collinearity at this point.

# **BUILD & SELECT MODELS**

## Logistic Regression Models

### **Model with All Predictors - AIC 7416.5**

* First, let's take a look at a binary logistic model with all variables included: 

```{r}
log_mod <- glm(TARGET_FLAG ~., data = df_insur_train[,-2],
               family = binomial(link = "logit"))
summary(log_mod)
```

```{r}
vif(log_mod)
```

* The full model above gives us an AIC of 7416.5, and indicates that using all the predictors does a better job predicting whether a person was in a car crash (TARGET_FLAG) than a null model with only the intercept (Residual deviance is less than the Null deviance).

* The degree of freedom adjusted variance inflation factors suggests that there is no concerning collinearity because all of the values are less than 3.

### **Model with Strongest Significant Predictors - AIC 8376.4**

* Next, let's explore a model with the predictors with the lowest p-values.  As shown above, there are 4 variable coefficients with significant p-values less than 0.0000000000000002 including CAR_USE, REVOKED, MVR_PTS, and URBANICITY.

```{r}
log_mod_2 <- glm(TARGET_FLAG ~CAR_USE + REVOKED + MVR_PTS + URBANICITY  , data = df_insur_train[,-2],
               family = binomial(link = "logit"))
summary(log_mod_2)
```

* The model above gives us an AIC of 8376.4, indicating that our initial model was a better fit given it's lower AIC of 7416.5.  Similarly we see that using these 4 predictors does a better job predicting whether a person was in a car crash (TARGET_FLAG) than a null model with only the intercept (Residual deviance is less than the Null deviance).

### **Backward Elimination Model - AIC 7408.4**

* As the model with all the predictors included was a better fit according to the AIC, we will use backward elimination to create an additional model for comparison.

```{r}
log_step <- step(log_mod, direction = "backward", test = "LRT")
```

```{r}
summary(log_step)
```

* This model above gives us an AIC of 7408.36, indicating that this model is a better fit than the others based on having the lowest AIC.  Backward Elimination leaves us with 17 variables including KIDSDRIV, HOMEKIDS, INCOME, PARENT, HOME_VAL, MSTATUS, EDUCATION, TRAVTIME, CAR_USE, BLUEBOOK, TIF, CAR_TYPE, OLDCLAIM, CLM_FREQ, REVOKED, MVR_PTS, and URBANICITY. As with the other models, the null model is outperformed as shown by the lower residual deviance compared to the null deviance.

The variables that positively impact the log odds of having car crash are the following:

* Kids driving
* Having kids at home (although this is a marginally significant p-value)
* Being a parent(vs not being a a parent)
* Having a longer travel time
* Having a car type other than minivan(when compared to minivan)
* Having an increased claims frequency 
* Having a revoked license
* Residing in an urban environment
* Having more points on the drivers license

The variables that negatively impact the log odds of having car crash are the following:

* Having a higher income
* Having a higher home value
* Being married
* Having a college of graduate level education as opposed to having less than a high school level education(there is no difference between having a high school diploma and not having one)
* Using the car for private as opposed to commercial use
* Having a higher Bluebook value for your vehicle
* Having a longer tenure as insurance client
* Having longer period of times between claims

### Assessing Model Performance

* We have selected the backward elimination model as our final Binary Logistic Regression Model for predicting a car crash given its better AIC.  First, we will predict the probabilities of a car crash using the final backward step-wise regression model from which we will then call the predicted car crash based on the probability of 0.5.

```{r}
df_insur_train$log_pred_prob <- predict(log_step,
                                        newdata = df_insur_train[,-c(1:2)],
                                        type = "response")
df_insur_train$log_pred <- ifelse(df_insur_train$log_pred_prob > 0.5, 1, 0)
```

* Next we will assess model performance by calculating the area under the curve (AUC) for this model.

```{r}
pROC::auc(df_insur_train$TARGET_FLAG, df_insur_train$log_pred)
pROC::roc(df_insur_train$TARGET_FLAG~df_insur_train$log_pred,
          plot = TRUE, print.auc = TRUE)
```

* The AUC of the model of .67 indicates that the model is only fair at predicting whether or not an insurance client will have a car crash.

* We can get a clearer sense of how the model under-performed by looking at a confusion matrix.

```{r}
confusionMatrix(as.factor(df_insur_train$log_pred),
                as.factor(df_insur_train$TARGET_FLAG), positive = "1")
```

* After fitting the final logistic model to the train data the accuracy obtained is 78.9%, but the sensitivity is extremely low at only 41% thus the balance accuracy is the same as the AUC at 66.8%. 
* It is worth noting that with such low sensitivity we can expect predictions to grossly under perform when predicting car crashes. 

### PREDICTING CAR CRASHES

* With the final logistic model, we will predict car crashes for the Evaluation data

```{r}
df_insur_eval$log_pred_prob <- predict(log_step,
                                       newdata = df_insur_eval[,-c(1:2)],
                                       type = "response")
df_insur_eval$log_pred <- ifelse(df_insur_eval$log_pred_prob > 0.5, 1, 0)
```

```{r}
df_insur_eval %>% 
  select(log_pred, KIDSDRIV, PARENT1, MSTATUS, EDUCATION, CAR_TYPE,
         REVOKED, URBANICITY, HOMEKIDS) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate_if(is.numeric, as.factor) %>% 
  pivot_longer(-log_pred, names_to = "key", values_to = "value") %>% 
  ggplot(aes(x = value, fill = log_pred)) + 
  geom_bar() + 
  scale_fill_discrete(labels = c("no crash", "crash"),
                      name = "Predicted Outcome") +
  coord_flip()+
  facet_wrap(~key, scales = "free")
```


```{r}
df_insur_eval %>% 
  select(log_pred, INCOME, HOME_VAL, TRAVTIME, TIF, OLDCLAIM,
         CLM_FREQ, MVR_PTS, BLUEBOOK) %>% 
  pivot_longer(-log_pred, names_to = "key", values_to = "value") %>%  
  ggplot(aes(y = value, x = as.factor(log_pred), fill = as.factor(log_pred))) + 
  geom_boxplot() + 
  scale_fill_discrete(labels = c("no crash", "crash"),
                      name = "Predicted Outcome") +
  xlab("Predicted Outcome")+
  facet_wrap(~key, scales = "free")
```

Assessing the predicted car crashes for the evaluation dataset, seems to largely reflect what was put into the model. Areas with stronger predictions were:

* Being a parent(vs not being a a parent)
* Having a longer travel time
* Having a car type other than minivan
* Having an increased claims frequency 
* Having a revoked license
* Residing in an urban environment
* Having a lower Bluebook value for your vehicle

We do not see any change in the predicted car crashes with respect to the variable home values. 

## Multiple Linear Regression Models

### **Model with All Predictors - Adj.R-Squared 0.06476**

* We will now be using Multiple Linear Regression to predict the cost if the person crashed their car (TARGET_AMT).

```{r}
mlr_mod <- lm(TARGET_AMT ~., data = df_insur_train[,-c(1,26:27)])
summary(mlr_mod)
```

```{r}
vif(mlr_mod)
```

* We can see the model with all the predictors, while overall significant, does a poor job in predicting cost as it can only account for 6.5% of the variability in the response variable TARGET_AMT.  There are only 17 of 31 significant variable coefficients.
* The degree of freedom adjusted variance inflation factors suggests that there is no concerning collinearity because all of the values are less than 3.

### **Square Root Transformed Model - Adj.R-Squared 0.1698**

* Let's see if a square root transformation of the numeric variables will make a better model 

```{r}
df_train_sqrt <- (df_insur_train) %>%
  mutate(TARGET_AMT = sqrt(TARGET_AMT)) %>%
  mutate(KIDSDRIV = sqrt(KIDSDRIV)) %>%
  mutate(AGE = sqrt(AGE)) %>%
  mutate(HOMEKIDS = sqrt(HOMEKIDS)) %>%
  mutate(YOJ = sqrt(YOJ)) %>%
  mutate(INCOME = sqrt(INCOME)) %>%
  mutate(HOME_VAL = sqrt(HOME_VAL)) %>%
  mutate(TRAVTIME = sqrt(TRAVTIME)) %>%
  mutate(BLUEBOOK = sqrt(BLUEBOOK)) %>%
  mutate(TIF = sqrt(TIF)) %>%
  mutate(OLDCLAIM = sqrt(OLDCLAIM)) %>%
  mutate(CLM_FREQ = sqrt(CLM_FREQ)) %>%
  mutate(MVR_PTS = sqrt(MVR_PTS)) %>%
  mutate(CAR_AGE = sqrt(CAR_AGE))
```

```{r}
mlr_mod_sqrt <- lm(TARGET_AMT ~., data = df_train_sqrt[,-c(1,26:27)])
summary(mlr_mod_sqrt)
```

* We can see transforming the numeric data using square root transformation did improve the model while remaining overall significant and increasing the adjusted r-squared from .0644 to .1698 and the number of significant variable coefficients from 17 to 23.

### **Backward Elimination - Adj.R-Squared .17**

* Our next model will use the square root transformed dataset and backward elimination.

```{r}
mlr_step <- step(mlr_mod_sqrt, direction = "backward", test = "F")
```

```{r}
mlr_final <- lm(TARGET_AMT ~ KIDSDRIV + HOMEKIDS + INCOME + PARENT1 + HOME_VAL + 
    MSTATUS + SEX + EDUCATION + JOB + TRAVTIME + CAR_USE + TIF + 
    CAR_TYPE + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + URBANICITY,
                data = df_train_sqrt[,-c(1, 26:27)])
```


```{r}
summary(mlr_final)
```

* We are selecting this backward elimination model on square root transformed data as our final Multiple Linear Regression Model as it has the greatest effect size (.17), is less complex given the smaller degrees of freedom (26), and has 24 of 26 significant variable coefficients.

The variables that positively impact the average cost of having car crash are the following:

* Kids driving
* Being male
* Being a parent(vs not being a a parent)
* Having a longer travel time
* Having a car type other than minivan(when compared to minivan)
* Having an increased claims frequency 
* Having white collar job
* Having a revoked license
* Residing in an urban environment
* Having higher points on drivers license 

The variable that negatively impact the average cost of having crash are the following:

* Having a higher income
* Being married
* Education beyond high school
* Using the car for private as opposed to commercial use
* Having a longer tenure as insurance client
* Having an older car

### Test Model Assumptions

```{r}
plot(mlr_final)
```

**1. Linearity** - the first plot shows that the relationship between target amount and the predictor variables in the final model is linear, so the assumption is net.

**2. Normality** - the second plot shows that our assumption of approximate normal distribution of the residuals may not be met due to the tails especially the right.

**3. Equality of Variances** - the third plot that there is unequal variance, however, the relationship is largely homoscedastic.

**4. Leverage / High Influence** - the fourth plot shows that there are a few outliers with very high claim

**Thus we should be cautious of this model given these issues with our model assumptions**

### Assessing Model Performance

* We are going to first predict the amount of the crash using the final model, we will then calculate the RMSE using the predictions.

```{r}
df_train_sqrt$mlr_pred <- predict(mlr_final,
                                   newdata = df_train_sqrt[,-c(1:2, 26:27)],
                                   type = "response")
RMSE(df_train_sqrt$mlr_pred, df_train_sqrt$TARGET_AMT)
summary(mlr_final)$adj.r.squared
```

* The RMSE for this model suggest an average deviation in the square root transformed predicted claim amount from the true claim amount of 31.2, which squared is 973.44. This suggests that the model is not doing a particularly good job at predicting accurate claim amounts. This is not surprising given that the R squared of the final model could only explain 17% of the total variation in the claim amount. 

### PREDICTING AMOUNT OF CLAIM

* With the final Multiple Linear Regression model, we will predict the amount of the claim for car crashes for the Evaluation dataset after performing the square root transformations.

```{r}
df_eval_sqrt <- (df_insur_eval) %>%
  mutate(TARGET_AMT = sqrt(TARGET_AMT)) %>%
  mutate(KIDSDRIV = sqrt(KIDSDRIV)) %>%
  mutate(AGE = sqrt(AGE)) %>%
  mutate(HOMEKIDS = sqrt(HOMEKIDS)) %>%
  mutate(YOJ = sqrt(YOJ)) %>%
  mutate(INCOME = sqrt(INCOME)) %>%
  mutate(HOME_VAL = sqrt(HOME_VAL)) %>%
  mutate(TRAVTIME = sqrt(TRAVTIME)) %>%
  mutate(BLUEBOOK = sqrt(BLUEBOOK)) %>%
  mutate(TIF = sqrt(TIF)) %>%
  mutate(OLDCLAIM = sqrt(OLDCLAIM)) %>%
  mutate(CLM_FREQ = sqrt(CLM_FREQ)) %>%
  mutate(MVR_PTS = sqrt(MVR_PTS)) %>%
  mutate(CAR_AGE = sqrt(CAR_AGE))
```

```{r}
df_eval_sqrt$mlr_pred <- predict(mlr_final,
                                  newdata = df_eval_sqrt[,-c(1:2, 26:27)],
                                  type = "response")

## Update TARGET_FLAG with the predicted values from our binary logistic regression
df_eval_sqrt$TARGET_FLAG <- df_insur_eval$log_pred

## Convert MLR prediction into original unit by squaring the values
df_eval_sqrt$TARGET_AMT <- ifelse(df_eval_sqrt$TARGET_FLAG ==1, df_eval_sqrt$mlr_pred^2, 0)

```

```{r}
coef(mlr_final)
```

```{r}
df_eval_sqrt %>% 
  select(TARGET_AMT, INCOME, HOME_VAL, TRAVTIME, TIF, CLM_FREQ, MVR_PTS,
         CAR_AGE) %>%
  mutate_if(is.character, as.numeric) %>% 
  pivot_longer(-TARGET_AMT, names_to = "key", values_to = "value") %>%  
  ggplot(aes(x = value, y = TARGET_AMT)) + 
  geom_point() + 
  geom_smooth() +
  ylab("Predicted Claim Amount USD")+
  facet_wrap(~key, scales = "free")
```

```{r}
df_eval_sqrt %>% 
  select(TARGET_AMT, KIDSDRIV, MSTATUS, CAR_TYPE, REVOKED, URBANICITY, CAR_USE,
         PARENT1) %>% 
  mutate_if(is.numeric, as.character) %>% 
  pivot_longer(-TARGET_AMT, names_to = "key", values_to = "value") %>% 
  mutate(TARGET_AMT = as.numeric(TARGET_AMT)) %>% 
  ggplot(aes(x = TARGET_AMT, y = value)) + 
  geom_boxplot(fill = "cadetblue") + 
  xlab("Predicted Claim Amount USD") +
  facet_wrap(~key, scales = "free")
```

```{r}
table(df_eval_sqrt$TARGET_FLAG)
sum(df_eval_sqrt$TARGET_AMT)

```

## **Conclusion**

* Using our binary logistic & multiple linear regression models, we predict that 414 of 1727 cases will have a car crash, which will amount to $663,690. However, given the under performance of the models on the training data and potential test assumption violations, we would be very cautious in using these predictions until additional variables & transformations could better improve the models.