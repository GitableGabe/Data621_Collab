}
tn_calc(actual, predicted)
fp_calc <- function(actual, predicted){
fp <- conf_mat(actual, predicted)[1, 2]
return(fp)
}
fp_calc(actual, predicted)
fn_calc <- function(actual, predicted){
fn <- conf_mat(actual, predicted)[2, 1]
return(fn)
}
fn_calc(actual, predicted)
accuracy_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp  <- tp_calc(actual, predicted)
tn  <- tn_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
fn  <- fn_calc(actual, predicted)
## Calculate accuracy
accuracy <- (tp + tn)/(tp + fp + tn + fn)
return(accuracy)
}
(accuracy <- accuracy_calc(df_classif, "class", "scored.class"))
class_error_rate <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp  <- tp_calc(actual, predicted)
tn  <- tn_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
fn  <- fn_calc(actual, predicted)
## Calculate classification error rate
classification_error_rate <- (fp + fn)/(tp + fp + tn + fn)
return(classification_error_rate)
}
(classification_error_rate <- class_error_rate(df_classif, "class", "scored.class"))
(accuracy + classification_error_rate)
precision_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp <- tp_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
## Calculate classification error rate
precision <- tp/(tp + fp)
return(precision)
}
(precision <- precision_calc(df_classif, "class", "scored.class"))
sensitivity_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp  <- tp_calc(actual, predicted)
fn  <- fn_calc(actual, predicted)
## Calculate classification error rate
sensitivity <- tp/(tp + fn)
return(sensitivity)
}
(sensitivity <-  sensitivity_calc(df_classif, "class", "scored.class"))
specificity_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tn  <- tn_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
## Calculate classification error rate
specificity <- tn/(tn + fp)
return(specificity)
}
(specificity <-  specificity_calc(df_classif, "class", "scored.class"))
f1 <- function(df){
## Call the previously defined functions
precision  <- precision_calc(df_classif, "class", "scored.class")
sensitivity  <- sensitivity_calc(df_classif, "class", "scored.class")
## Calculate F1 score
f1_score <- (2 * precision * sensitivity)/(precision + sensitivity)
return(f1_score)
}
(f1_score <-  f1(df_classif))
precision_seq <- seq(0, 1, length.out = 25)
f1_df <- data.frame(precision_seq)
# to calculate f1 using varying  precision and sensitivity = 50%
f1_df <- f1_df %>%
mutate(f1_50 = (2 * precision_seq * 0.50)/(precision_seq + 0.50))
# to repeat for sensitivity 1, 25, 75, 99 percent
f1_df <- f1_df %>%
mutate(f1_1 = (2 * precision_seq * 0.01)/(precision_seq + 0.01),
f1_25 = (2 * precision_seq * 0.25)/(precision_seq + 0.25),
f1_75 = (2 * precision_seq * 0.75)/(precision_seq + 0.75),
f1_99 = (2 * precision_seq * 0.99)/(precision_seq + 0.99))
head(f1_df)
f1_df %>% pivot_longer(cols = -precision_seq, names_to = "Sensitivity", names_prefix = "f1_", values_to = "f1") %>%
ggplot(aes(x = precision_seq, y = f1, color = Sensitivity)) +
geom_line() +
labs(y = "F1", x = "Precision", title = "F1 by Varying Precision and Sensitivity", subtitle = "F1 Bounded by 0 and 1")
actual <- ls_class
predicted <- ls_scr_class
AUC_calc <- function (actual, predicted){
df <- data.frame(actual = actual, predicted = predicted)
# Calculate total number of pairs to check - permutation of how many 1's and 0's exist in the actual column
totalPairs <- nrow(subset(df, actual == "1")) * nrow(subset(df, actual == "0"))
# Calculate concordance = number of pairs where actual and predicted AGREE
df <- df %>% mutate(agreement = ifelse(actual == predicted, 1, 0))
# Calculate discordance = number of pairs where actual and predicted DISAGREE
df <- df %>% mutate(disagreement = ifelse(actual != predicted, 1, 0))
conc <- sum(df$agreement)
sum(df$disagreement)
conc <- c(vapply(ones$Predicted,
function(x) {
((x > zeros$Predicted))
},
FUN.VALUE=logical(nrow(zeros))))
# disc <- sum(c(vapply(ones$Predicted,
#                      function(x) {((x < zeros$Predicted))},
#                      FUN.VALUE = logical(nrow(zeros)))), na.rm = T)
concordance <- conc/nrow(df)
discordance <- disc/totalPairs
tiesPercent <- (1-concordance-discordance)
AUC = concordance + 0.5*tiesPercent
return(list("Concordance"=concordance, "Discordance"=discordance,
"Tied"=tiesPercent, "AUC"=AUC))
}
auc(as.numeric(actual), as.numeric(predicted))
nrow(df_classif)
paste("CONFUSION MATRIX")
conf_mat(actual, predicted)
paste("True positives:")
tp_calc(actual, predicted)
paste("True negatives:")
tn_calc(actual, predicted)
paste("False positives:")
fp_calc(actual, predicted)
paste("False negatives:")
fn_calc(actual, predicted)
paste("Accuracy:")
accuracy_calc(df_classif, "class", "scored.class")
paste("Precision:")
precision_calc(df_classif, "class", "scored.class")
paste("Calssificaiton Error Rate:")
class_error_rate(df_classif, "class", "scored.class")
paste("Specificity:")
specificity_calc(df_classif, "class", "scored.class")
paste("Sensitivity:")
sensitivity_calc(df_classif, "class", "scored.class")
paste("F1:")
f1(df_classif)
confusionMatrix(data=ls_scr_class, reference = ls_class)
roc(as.numeric(actual), as.numeric(predicted), plot = TRUE, print.auc = TRUE)
# Create a ROC curve object
roc_curve <- roc(ls_class, ls_sr_prb)
# Plot the ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
# Calculate AUC
auc_value <- auc(roc_curve)
# Create a ROC curve object
roc_curve <- roc(ls_class, ls_sr_prb)
# Plot the ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
# Calculate AUC
auc_value <- auc(roc_curve)
(result_list <- list(roc_curve_plot = roc_curve, auc_value = auc_value))
result_list <- list(roc_curve_plot = roc_curve, auc_value = auc_value)
(result_list <- list(roc_curve_plot = roc_curve, auc_value = auc_value))
# Calculate True Positive Rate (Sensitivity) and False Positive Rate
tpr <- sensitivity
fpr <- 1 - specificity
# Plot the ROC curve
plot(fpr, tpr, type = 'l', col = 'blue', lwd = 2,
main = 'ROC Curve', xlab = 'False Positive Rate', ylab = 'True Positive Rate')
# Calculate True Positive Rate (Sensitivity) and False Positive Rate
tpr <- sensitivity_calc
fpr <-1 - specificity_calc
# Calculate True Positive Rate (Sensitivity) and False Positive Rate
tpr <- sensitivity_calc
fpr <-(1 - specificity_calc)
# Calculate True Positive Rate (Sensitivity) and False Positive Rate
tpr <- sensitivity_calc
fpr <-specificity_calc
# Plot the ROC curve
plot(fpr, tpr, type = 'l', col = 'blue', lwd = 2,
main = 'ROC Curve', xlab = 'False Positive Rate', ylab = 'True Positive Rate')
# Calculate True Positive Rate (Sensitivity) and False Positive Rate
tpr <- sensitivity_calc
fpr <- specificity_calc
# Plot the ROC curve
plot(fpr, tpr, type = 'l', col = 'blue', lwd = 2,
main = 'ROC Curve', xlab = 'False Positive Rate', ylab = 'True Positive Rate')
# Function to generate ROC curve without external packages
generate_ROC_curve <- function(df_classif, ls_class, ls_sr_prb) {
# Sort data by predicted probabilities in descending order
sorted_data <- df_classif[order(-df_classif[[ls_sr_prb]]), ]
# Initialize variables
tp <- 0  # True Positives
fp <- 0  # False Positives
tn <- sum(df_classif[[ls_class]] == 0)  # True Negatives
fn <- sum(df_classif[[ls_class]] == 1)  # False Negatives
# Initialize vectors to store True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr_vector <- numeric()
fpr_vector <- numeric()
# Iterate through sorted data
for (i in 1:nrow(sorted_data)) {
if (sorted_data[[true_class_col]][i] == 1) {
tp <- tp + 1
fn <- fn - 1
} else {
fp <- fp + 1
tn <- tn - 1
}
# Calculate True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr <- tp / (tp + fn)
fpr <- fp / (fp + tn)
# Append to vectors
tpr_vector <- c(tpr_vector, tpr)
fpr_vector <- c(fpr_vector, fpr)
}
# Calculate AUC using trapezoidal rule
auc <- sum(diff(fpr_vector) * tpr_vector[-1])
# Plot ROC curve
plot(fpr_vector, tpr_vector, type = "l", col = "blue", lwd = 2,
main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
abline(a = 0, b = 1, col = "red", lty = 2)
# Return AUC value
return(auc)
}
# Example usage
# Assuming your data frame is named 'my_data' with columns 'class' and 'scored.probability'
auc_value <- generate_ROC_curve(my_data, "class", "scored.probability")
# Function to generate ROC curve without external packages
generate_ROC_curve <- function(data, true_class_col, prob_col) {
# Sort data by predicted probabilities in descending order
sorted_data <- data[order(-data[[prob_col]]), ]
# Initialize variables
tp <- 0  # True Positives
fp <- 0  # False Positives
tn <- sum(data[[true_class_col]] == 0)  # True Negatives
fn <- sum(data[[true_class_col]] == 1)  # False Negatives
# Initialize vectors to store True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr_vector <- numeric()
fpr_vector <- numeric()
# Iterate through sorted data
for (i in 1:nrow(sorted_data)) {
if (sorted_data[[true_class_col]][i] == 1) {
tp <- tp + 1
fn <- fn - 1
} else {
fp <- fp + 1
tn <- tn - 1
}
# Calculate True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr <- tp / (tp + fn)
fpr <- fp / (fp + tn)
# Append to vectors
tpr_vector <- c(tpr_vector, tpr)
fpr_vector <- c(fpr_vector, fpr)
}
# Calculate AUC using trapezoidal rule
auc <- sum(diff(fpr_vector) * tpr_vector[-1])
# Plot ROC curve
plot(fpr_vector, tpr_vector, type = "l", col = "blue", lwd = 2,
main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
abline(a = 0, b = 1, col = "red", lty = 2)
# Return AUC value
return(auc)
}
# Example usage
# Assuming your data frame is named 'my_data' with columns 'class' and 'scored.probability'
auc_value <- generate_ROC_curve(df_classif, ls_class, ls_sr_prb)
# Function to generate ROC curve without external packages
generate_ROC_curve <- function(data, true_class_col, prob_col) {
# Sort data by predicted probabilities in descending order
sorted_data <- data[order(-data[[prob_col]]), ]
# Initialize variables
tp <- 0  # True Positives
fp <- 0  # False Positives
tn <- sum(data[[true_class_col]] == 0)  # True Negatives
fn <- sum(data[[true_class_col]] == 1)  # False Negatives
# Initialize vectors to store True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr_vector <- numeric()
fpr_vector <- numeric()
# Iterate through sorted data
for (i in 1:nrow(sorted_data)) {
if (sorted_data[[true_class_col]][i] == 1) {
tp <- tp + 1
fn <- fn - 1
} else {
fp <- fp + 1
tn <- tn - 1
}
# Calculate True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr <- tp / (tp + fn)
fpr <- fp / (fp + tn)
# Append to vectors
tpr_vector <- c(tpr_vector, tpr)
fpr_vector <- c(fpr_vector, fpr)
}
# Calculate AUC using trapezoidal rule
auc <- sum(diff(fpr_vector) * tpr_vector[-1])
# Plot ROC curve
plot(fpr_vector, tpr_vector, type = "l", col = "blue", lwd = 2,
main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
abline(a = 0, b = 1, col = "red", lty = 2)
# Return AUC value
return(auc)
}
# Example usage
# Assuming your data frame is named 'my_data' with columns 'class' and 'scored.probability'
auc_value <- generate_ROC_curve(df_classif, "class", "scored.probability")
cat("AUC:", auc_value, "\n")
# Function to generate ROC curve without external packages
generate_ROC_curve <- function(data, true_class_col, prob_col) {
# Sort data by predicted probabilities in descending order
sorted_data <- data[order(-data[[prob_col]]), ]
# Initialize variables
tp <- 0  # True Positives
fp <- 0  # False Positives
tn <- sum(data[[true_class_col]] == 1)  # True Negatives
fn <- sum(data[[true_class_col]] == 0)  # False Negatives
# Initialize vectors to store True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr_vector <- numeric()
fpr_vector <- numeric()
# Iterate through sorted data
for (i in 1:nrow(sorted_data)) {
if (sorted_data[[true_class_col]][i] == 1) {
tp <- tp + 1
fn <- fn - 1
} else {
fp <- fp + 1
tn <- tn - 1
}
# Calculate True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr <- tp / (tp + fn)
fpr <- fp / (fp + tn)
# Append to vectors
tpr_vector <- c(tpr_vector, tpr)
fpr_vector <- c(fpr_vector, fpr)
}
# Calculate AUC using trapezoidal rule
auc <- sum(diff(fpr_vector) * tpr_vector[-1])
# Plot ROC curve
plot(fpr_vector, tpr_vector, type = "l", col = "blue", lwd = 2,
main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
abline(a = 0, b = 1, col = "red", lty = 2)
# Return AUC value
return(auc)
}
# Example usage
# Assuming your data frame is named 'my_data' with columns 'class' and 'scored.probability'
auc_value <- generate_ROC_curve(df_classif, "class", "scored.probability")
cat("AUC:", auc_value, "\n")
# Function to generate ROC curve without external packages
generate_ROC_curve <- function(data, true_class_col, prob_col) {
# Sort data by predicted probabilities in descending order
sorted_data <- data[order(-data[[prob_col]]), ]
# Initialize variables
tp <- 0  # True Positives
fp <- 0  # False Positives
tn <- sum(data[[true_class_col]] == 0)  # True Negatives
fn <- sum(data[[true_class_col]] == 1)  # False Negatives
# Initialize vectors to store True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr_vector <- numeric()
fpr_vector <- numeric()
# Iterate through sorted data
for (i in 1:nrow(sorted_data)) {
if (sorted_data[[true_class_col]][i] == 1) {
tp <- tp + 1
fn <- fn - 1
} else {
fp <- fp + 1
tn <- tn - 1
}
# Calculate True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr <- tp / (tp + fn)
fpr <- fp / (fp + tn)
# Append to vectors
tpr_vector <- c(tpr_vector, tpr)
fpr_vector <- c(fpr_vector, fpr)
}
# Calculate AUC using trapezoidal rule
auc <- sum(diff(fpr_vector) * tpr_vector[-1])
# Plot ROC curve
plot(fpr_vector, tpr_vector, type = "l", col = "blue", lwd = 2,
main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
abline(a = 0, b = 1, col = "red", lty = 2)
# Return AUC value
return(auc)
}
# Example usage
# Assuming your data frame is named 'my_data' with columns 'class' and 'scored.probability'
auc_value <- generate_ROC_curve(df_classif, "class", "scored.probability")
cat("AUC:", auc_value, "\n")
# Function to generate ROC curve without external packages
generate_ROC_curve <- function(data, true_class_col, prob_col) {
# Sort data by predicted probabilities in descending order
sorted_data <- data[order(-data[[prob_col]]), ]
# Initialize variables
tp <- 0  # True Positives
fp <- 0  # False Positives
tn <- sum(data[[true_class_col]] == 1)  # True Negatives
fn <- sum(data[[true_class_col]] == 0)  # False Negatives
# Initialize vectors to store True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr_vector <- numeric()
fpr_vector <- numeric()
# Iterate through sorted data
for (i in 1:nrow(sorted_data)) {
if (sorted_data[[true_class_col]][i] == 1) {
tp <- tp + 1
fn <- fn - 1
} else {
fp <- fp + 1
tn <- tn - 1
}
# Calculate True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr <- tp / (tp + fn)
fpr <- fp / (fp + tn)
# Append to vectors
tpr_vector <- c(tpr_vector, tpr)
fpr_vector <- c(fpr_vector, fpr)
}
# Calculate AUC using trapezoidal rule
auc <- sum(diff(fpr_vector) * tpr_vector[-1])
# Plot ROC curve
plot(fpr_vector, tpr_vector, type = "l", col = "blue", lwd = 2,
main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
abline(a = 0, b = 1, col = "red", lty = 2)
# Return AUC value
return(auc)
}
# Example usage
# Assuming your data frame is named 'my_data' with columns 'class' and 'scored.probability'
auc_value <- generate_ROC_curve(df_classif, "class", "scored.probability")
cat("AUC:", auc_value, "\n")
# Function to generate ROC curve without external packages
generate_ROC_curve <- function(data, true_class_col, prob_col) {
# Sort data by predicted probabilities in descending order
sorted_data <- data[order(-data[[prob_col]]), ]
# Initialize variables
tp <- 0  # True Positives
fp <- 0  # False Positives
tn <- sum(data[[true_class_col]] == 0)  # True Negatives
fn <- sum(data[[true_class_col]] == 1)  # False Negatives
# Initialize vectors to store True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr_vector <- numeric()
fpr_vector <- numeric()
# Iterate through sorted data
for (i in 1:nrow(sorted_data)) {
if (sorted_data[[true_class_col]][i] == 1) {
tp <- tp + 1
fn <- fn - 1
} else {
fp <- fp + 1
tn <- tn - 1
}
# Calculate True Positive Rate (Sensitivity) and False Positive Rate (1-Specificity)
tpr <- tp / (tp + fn)
fpr <- fp / (fp + tn)
# Append to vectors
tpr_vector <- c(tpr_vector, tpr)
fpr_vector <- c(fpr_vector, fpr)
}
# Calculate AUC using trapezoidal rule
auc <- sum(diff(fpr_vector) * tpr_vector[-1])
# Plot ROC curve
plot(fpr_vector, tpr_vector, type = "l", col = "blue", lwd = 2,
main = "ROC Curve", xlab = "False Positive Rate", ylab = "True Positive Rate")
abline(a = 0, b = 1, col = "red", lty = 2)
# Return AUC value
return(auc)
}
# Example usage
# Assuming your data frame is named 'my_data' with columns 'class' and 'scored.probability'
auc_value <- generate_ROC_curve(df_classif, "class", "scored.probability")
cat("AUC:", auc_value, "\n")
>>>>>>> Stashed changes
library(dplyr)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(GGally)
library(Hmisc)
library(corrplot)
library(PerformanceAnalytics)
git_url<-
"https://raw.githubusercontent.com/GitableGabe/Data621_Data/main/"
df_crime_eval <-
read.csv(paste0(git_url,"crime-evaluation-data_modified.csv"))
head(df_crime_eval,n=10)
df_crime_train <-
read.csv(paste0(git_url,"crime-training-data_modified.csv"))
head(df_crime_train,n=10)
ggcorr(df_crime_train)
#ggpairs(df_crime_train, columns = c(1, 2, 4:9, 11, 12), aes(alpha = 0.5), diag = list(continuous = "blankDiag")) +
#theme_bw()
#pairs(train[, c(1, 2, 4:9, 11, 12)], col = "darkgrey", gap = 0, cex.labels = 1.2)
corrplot(cor(df_crime_train[, c(1, 2, 4:9, 11, 12)]), type = "lower")
coll <- rcorr(round(cor(df_crime_train[, c(1, 2, 4:9, 11, 12)]), digits = 2))
round(coll$P, digits = 3)
#chart.Correlation(train[, c(1, 2, 4:9, 11, 12)], histogram = FALSE, lower.panel = NULL)
df_crime_train %>%
pivot_longer(cols = !target, names_to = "predictor", values_to = "value") %>%
ggplot(aes(x = as.factor(target), y = value, fill = as.factor(target))) +
geom_boxplot(show.legend = FALSE) +
xlab("target") +
facet_wrap(~predictor, scales = "free")
df_crime_train %>%
pivot_longer(cols = !target, names_to = "predictor", values_to = "value") %>%
group_by(target) %>%
count()
View(df_crime_train)
summary(df_crime_train)
summary(df_crime_eval)
