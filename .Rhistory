predicted <- relevel(ls_scr_class, ref = "1")
confusion_matrix <- table(predicted, actual)
return(confusion_matrix)
}
conf_mat(actual, predicted)
tp_calc <- function(actual, predicted){
tp <- conf_mat(actual, predicted)[1, 1]
return(tp)
}
tp_calc(actual, predicted)
tn_calc <- function(actual, predicted){
tn <- conf_mat(actual, predicted)[2, 2]
return(tn)
}
tn_calc(actual, predicted)
fp_calc <- function(actual, predicted){
fp <- conf_mat(actual, predicted)[1, 2]
return(fp)
}
fp_calc(actual, predicted)
fn_calc <- function(actual, predicted){
fn <- conf_mat(actual, predicted)[2, 1]
return(fn)
}
fn_calc(actual, predicted)
accuracy_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp  <- tp_calc(actual, predicted)
tn  <- tn_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
fn  <- fn_calc(actual, predicted)
## Calculate accuracy
accuracy <- (tp + tn)/(tp + fp + tn + fn)
return(accuracy)
}
(accuracy <- accuracy_calc(df_classif, "class", "scored.class"))
class_error_rate <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp  <- tp_calc(actual, predicted)
tn  <- tn_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
fn  <- fn_calc(actual, predicted)
## Calculate classification error rate
classification_error_rate <- (fp + fn)/(tp + fp + tn + fn)
return(classification_error_rate)
}
(classification_error_rate <- class_error_rate(df_classif, "class", "scored.class"))
(accuracy + classification_error_rate)
precision_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp <- tp_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
## Calculate classification error rate
precision <- tp/(tp + fp)
return(precision)
}
(precision <- precision_calc(df_classif, "class", "scored.class"))
sensitivity_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp  <- tp_calc(actual, predicted)
fn  <- fn_calc(actual, predicted)
## Calculate classification error rate
sensitivity <- tp/(tp + fn)
return(sensitivity)
}
(sensitivity <-  sensitivity_calc(df_classif, "class", "scored.class"))
specificity_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tn  <- tn_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
## Calculate classification error rate
specificity <- tn/(tn + fp)
return(specificity)
}
(specificity <-  specificity_calc(df_classif, "class", "scored.class"))
f1 <- function(df){
## Call the previously defined functions
precision  <- precision_calc(df_classif, "class", "scored.class")
sensitivity  <- sensitivity_calc(df_classif, "class", "scored.class")
## Calculate F1 score
f1_score <- (2 * precision * sensitivity)/(precision + sensitivity)
return(f1_score)
}
(f1_score <-  f1(df_classif))
precision_seq <- seq(0, 1, length.out = 25)
f1_df <- data.frame(precision_seq)
# to calculate f1 using varying  precision and sensitivity = 50%
f1_df <- f1_df %>%
mutate(f1_50 = (2 * precision_seq * 0.50)/(precision_seq + 0.50))
# to repeat for sensitivity 1, 25, 75, 99 percent
f1_df <- f1_df %>%
mutate(f1_1 = (2 * precision_seq * 0.01)/(precision_seq + 0.01),
f1_25 = (2 * precision_seq * 0.25)/(precision_seq + 0.25),
f1_75 = (2 * precision_seq * 0.75)/(precision_seq + 0.75),
f1_99 = (2 * precision_seq * 0.99)/(precision_seq + 0.99))
head(f1_df)
f1_df %>% pivot_longer(cols = -precision_seq, names_to = "Sensitivity", names_prefix = "f1_", values_to = "f1") %>%
ggplot(aes(x = precision_seq, y = f1, color = Sensitivity)) +
geom_line() +
labs(y = "F1", x = "Precision", title = "F1 by Varying Precision and Sensitivity", subtitle = "F1 Bounded by 0 and 1")
actual <- ls_class
predicted <- ls_scr_class
AUC_calc <- function (actual, predicted){
df <- data.frame(actual = actual, predicted = predicted)
# Calculate total number of pairs to check - permutation of how many 1's and 0's exist in the actual column
totalPairs <- nrow(subset(df, actual == "1")) * nrow(subset(df, actual == "0"))
# Calculate concordance = number of pairs where actual and predicted AGREE
df <- df %>% mutate(agreement = ifelse(actual == predicted, 1, 0))
# Calculate discordance = number of pairs where actual and predicted DISAGREE
df <- df %>% mutate(disagreement = ifelse(actual != predicted, 1, 0))
conc <- sum(df$agreement)
sum(df$disagreement)
conc <- c(vapply(ones$Predicted,
function(x) {
((x > zeros$Predicted))
},
FUN.VALUE=logical(nrow(zeros))))
# disc <- sum(c(vapply(ones$Predicted,
#                      function(x) {((x < zeros$Predicted))},
#                      FUN.VALUE = logical(nrow(zeros)))), na.rm = T)
concordance <- conc/nrow(df)
discordance <- disc/totalPairs
tiesPercent <- (1-concordance-discordance)
AUC = concordance + 0.5*tiesPercent
return(list("Concordance"=concordance, "Discordance"=discordance,
"Tied"=tiesPercent, "AUC"=AUC))
}
AUC(ls_class, ls_scr_class)
install.packages('pROC')
install.packages("pROC")
install.packages('pROC')
library(dplyr)
library(tidyr)
library(caret)
library(ggplot2)
library(pROC)
git_url<-
"https://raw.githubusercontent.com/GitableGabe/Data621_Data/main/"
df_classif <-
read.csv(paste0(git_url,"classification-output-data.csv"))
# head(df_classif,n=10)
ls_class<-factor(df_classif$class)
ls_scr_class<-factor(df_classif$scored.class)
ls_sr_prb<-df_classif$scored.probability
# let's set the positive outcome to "1" with relevel
actual <- relevel(ls_class, ref = "1") ## changes it from the default ref of 0
predicted <- relevel(ls_scr_class, ref = "1")
conf_mat <- function(actual, predicted){
## Have to relevel again within the function
actual <- relevel(ls_class, ref = "1") ## changes it from the default ref of 0
predicted <- relevel(ls_scr_class, ref = "1")
confusion_matrix <- table(predicted, actual)
return(confusion_matrix)
}
conf_mat(actual, predicted)
tp_calc <- function(actual, predicted){
tp <- conf_mat(actual, predicted)[1, 1]
return(tp)
}
tp_calc(actual, predicted)
tn_calc <- function(actual, predicted){
tn <- conf_mat(actual, predicted)[2, 2]
return(tn)
}
tn_calc(actual, predicted)
fp_calc <- function(actual, predicted){
fp <- conf_mat(actual, predicted)[1, 2]
return(fp)
}
fp_calc(actual, predicted)
fn_calc <- function(actual, predicted){
fn <- conf_mat(actual, predicted)[2, 1]
return(fn)
}
fn_calc(actual, predicted)
accuracy_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp  <- tp_calc(actual, predicted)
tn  <- tn_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
fn  <- fn_calc(actual, predicted)
## Calculate accuracy
accuracy <- (tp + tn)/(tp + fp + tn + fn)
return(accuracy)
}
(accuracy <- accuracy_calc(df_classif, "class", "scored.class"))
class_error_rate <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp  <- tp_calc(actual, predicted)
tn  <- tn_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
fn  <- fn_calc(actual, predicted)
## Calculate classification error rate
classification_error_rate <- (fp + fn)/(tp + fp + tn + fn)
return(classification_error_rate)
}
(classification_error_rate <- class_error_rate(df_classif, "class", "scored.class"))
(accuracy + classification_error_rate)
precision_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp <- tp_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
## Calculate classification error rate
precision <- tp/(tp + fp)
return(precision)
}
(precision <- precision_calc(df_classif, "class", "scored.class"))
sensitivity_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp  <- tp_calc(actual, predicted)
fn  <- fn_calc(actual, predicted)
## Calculate classification error rate
sensitivity <- tp/(tp + fn)
return(sensitivity)
}
(sensitivity <-  sensitivity_calc(df_classif, "class", "scored.class"))
specificity_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tn  <- tn_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
## Calculate classification error rate
specificity <- tn/(tn + fp)
return(specificity)
}
(specificity <-  specificity_calc(df_classif, "class", "scored.class"))
f1 <- function(df){
## Call the previously defined functions
precision  <- precision_calc(df_classif, "class", "scored.class")
sensitivity  <- sensitivity_calc(df_classif, "class", "scored.class")
## Calculate F1 score
f1_score <- (2 * precision * sensitivity)/(precision + sensitivity)
return(f1_score)
}
(f1_score <-  f1(df_classif))
precision_seq <- seq(0, 1, length.out = 25)
f1_df <- data.frame(precision_seq)
# to calculate f1 using varying  precision and sensitivity = 50%
f1_df <- f1_df %>%
mutate(f1_50 = (2 * precision_seq * 0.50)/(precision_seq + 0.50))
# to repeat for sensitivity 1, 25, 75, 99 percent
f1_df <- f1_df %>%
mutate(f1_1 = (2 * precision_seq * 0.01)/(precision_seq + 0.01),
f1_25 = (2 * precision_seq * 0.25)/(precision_seq + 0.25),
f1_75 = (2 * precision_seq * 0.75)/(precision_seq + 0.75),
f1_99 = (2 * precision_seq * 0.99)/(precision_seq + 0.99))
head(f1_df)
f1_df %>% pivot_longer(cols = -precision_seq, names_to = "Sensitivity", names_prefix = "f1_", values_to = "f1") %>%
ggplot(aes(x = precision_seq, y = f1, color = Sensitivity)) +
geom_line() +
labs(y = "F1", x = "Precision", title = "F1 by Varying Precision and Sensitivity", subtitle = "F1 Bounded by 0 and 1")
actual <- ls_class
predicted <- ls_scr_class
AUC_calc <- function (actual, predicted){
df <- data.frame(actual = actual, predicted = predicted)
# Calculate total number of pairs to check - permutation of how many 1's and 0's exist in the actual column
totalPairs <- nrow(subset(df, actual == "1")) * nrow(subset(df, actual == "0"))
# Calculate concordance = number of pairs where actual and predicted AGREE
df <- df %>% mutate(agreement = ifelse(actual == predicted, 1, 0))
# Calculate discordance = number of pairs where actual and predicted DISAGREE
df <- df %>% mutate(disagreement = ifelse(actual != predicted, 1, 0))
conc <- sum(df$agreement)
sum(df$disagreement)
conc <- c(vapply(ones$Predicted,
function(x) {
((x > zeros$Predicted))
},
FUN.VALUE=logical(nrow(zeros))))
# disc <- sum(c(vapply(ones$Predicted,
#                      function(x) {((x < zeros$Predicted))},
#                      FUN.VALUE = logical(nrow(zeros)))), na.rm = T)
concordance <- conc/nrow(df)
discordance <- disc/totalPairs
tiesPercent <- (1-concordance-discordance)
AUC = concordance + 0.5*tiesPercent
return(list("Concordance"=concordance, "Discordance"=discordance,
"Tied"=tiesPercent, "AUC"=AUC))
}
auc(ls_class, ls_scr_class)
actual <- ls_class
predicted <- ls_scr_class
AUC_calc <- function (actual, predicted){
df <- data.frame(actual = actual, predicted = predicted)
# Calculate total number of pairs to check - permutation of how many 1's and 0's exist in the actual column
totalPairs <- nrow(subset(df, actual == "1")) * nrow(subset(df, actual == "0"))
# Calculate concordance = number of pairs where actual and predicted AGREE
df <- df %>% mutate(agreement = ifelse(actual == predicted, 1, 0))
# Calculate discordance = number of pairs where actual and predicted DISAGREE
df <- df %>% mutate(disagreement = ifelse(actual != predicted, 1, 0))
conc <- sum(df$agreement)
sum(df$disagreement)
conc <- c(vapply(ones$Predicted,
function(x) {
((x > zeros$Predicted))
},
FUN.VALUE=logical(nrow(zeros))))
# disc <- sum(c(vapply(ones$Predicted,
#                      function(x) {((x < zeros$Predicted))},
#                      FUN.VALUE = logical(nrow(zeros)))), na.rm = T)
concordance <- conc/nrow(df)
discordance <- disc/totalPairs
tiesPercent <- (1-concordance-discordance)
AUC = concordance + 0.5*tiesPercent
return(list("Concordance"=concordance, "Discordance"=discordance,
"Tied"=tiesPercent, "AUC"=AUC))
}
auc(ls_class, ls_scr_class)
library(dplyr)
library(tidyr)
library(caret)
library(ggplot2)
library(pROC)
git_url<-
"https://raw.githubusercontent.com/GitableGabe/Data621_Data/main/"
df_classif <-
read.csv(paste0(git_url,"classification-output-data.csv"))
# head(df_classif,n=10)
ls_class<-factor(df_classif$class)
ls_scr_class<-factor(df_classif$scored.class)
ls_sr_prb<-df_classif$scored.probability
# let's set the positive outcome to "1" with relevel
actual <- relevel(ls_class, ref = "1") ## changes it from the default ref of 0
predicted <- relevel(ls_scr_class, ref = "1")
conf_mat <- function(actual, predicted){
## Have to relevel again within the function
actual <- relevel(ls_class, ref = "1") ## changes it from the default ref of 0
predicted <- relevel(ls_scr_class, ref = "1")
confusion_matrix <- table(predicted, actual)
return(confusion_matrix)
}
conf_mat(actual, predicted)
tp_calc <- function(actual, predicted){
tp <- conf_mat(actual, predicted)[1, 1]
return(tp)
}
tp_calc(actual, predicted)
tn_calc <- function(actual, predicted){
tn <- conf_mat(actual, predicted)[2, 2]
return(tn)
}
tn_calc(actual, predicted)
fp_calc <- function(actual, predicted){
fp <- conf_mat(actual, predicted)[1, 2]
return(fp)
}
fp_calc(actual, predicted)
fn_calc <- function(actual, predicted){
fn <- conf_mat(actual, predicted)[2, 1]
return(fn)
}
fn_calc(actual, predicted)
accuracy_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp  <- tp_calc(actual, predicted)
tn  <- tn_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
fn  <- fn_calc(actual, predicted)
## Calculate accuracy
accuracy <- (tp + tn)/(tp + fp + tn + fn)
return(accuracy)
}
(accuracy <- accuracy_calc(df_classif, "class", "scored.class"))
class_error_rate <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp  <- tp_calc(actual, predicted)
tn  <- tn_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
fn  <- fn_calc(actual, predicted)
## Calculate classification error rate
classification_error_rate <- (fp + fn)/(tp + fp + tn + fn)
return(classification_error_rate)
}
(classification_error_rate <- class_error_rate(df_classif, "class", "scored.class"))
(accuracy + classification_error_rate)
precision_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp <- tp_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
## Calculate classification error rate
precision <- tp/(tp + fp)
return(precision)
}
(precision <- precision_calc(df_classif, "class", "scored.class"))
sensitivity_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tp  <- tp_calc(actual, predicted)
fn  <- fn_calc(actual, predicted)
## Calculate classification error rate
sensitivity <- tp/(tp + fn)
return(sensitivity)
}
(sensitivity <-  sensitivity_calc(df_classif, "class", "scored.class"))
specificity_calc <- function(df, col1, col2){
actual <- df[,col1]
predicted <- df[,col2]
## Call the previously defined functions
tn  <- tn_calc(actual, predicted)
fp  <- fp_calc(actual, predicted)
## Calculate classification error rate
specificity <- tn/(tn + fp)
return(specificity)
}
(specificity <-  specificity_calc(df_classif, "class", "scored.class"))
f1 <- function(df){
## Call the previously defined functions
precision  <- precision_calc(df_classif, "class", "scored.class")
sensitivity  <- sensitivity_calc(df_classif, "class", "scored.class")
## Calculate F1 score
f1_score <- (2 * precision * sensitivity)/(precision + sensitivity)
return(f1_score)
}
(f1_score <-  f1(df_classif))
precision_seq <- seq(0, 1, length.out = 25)
f1_df <- data.frame(precision_seq)
# to calculate f1 using varying  precision and sensitivity = 50%
f1_df <- f1_df %>%
mutate(f1_50 = (2 * precision_seq * 0.50)/(precision_seq + 0.50))
# to repeat for sensitivity 1, 25, 75, 99 percent
f1_df <- f1_df %>%
mutate(f1_1 = (2 * precision_seq * 0.01)/(precision_seq + 0.01),
f1_25 = (2 * precision_seq * 0.25)/(precision_seq + 0.25),
f1_75 = (2 * precision_seq * 0.75)/(precision_seq + 0.75),
f1_99 = (2 * precision_seq * 0.99)/(precision_seq + 0.99))
head(f1_df)
f1_df %>% pivot_longer(cols = -precision_seq, names_to = "Sensitivity", names_prefix = "f1_", values_to = "f1") %>%
ggplot(aes(x = precision_seq, y = f1, color = Sensitivity)) +
geom_line() +
labs(y = "F1", x = "Precision", title = "F1 by Varying Precision and Sensitivity", subtitle = "F1 Bounded by 0 and 1")
actual <- ls_class
predicted <- ls_scr_class
AUC_calc <- function (actual, predicted){
df <- data.frame(actual = actual, predicted = predicted)
# Calculate total number of pairs to check - permutation of how many 1's and 0's exist in the actual column
totalPairs <- nrow(subset(df, actual == "1")) * nrow(subset(df, actual == "0"))
# Calculate concordance = number of pairs where actual and predicted AGREE
df <- df %>% mutate(agreement = ifelse(actual == predicted, 1, 0))
# Calculate discordance = number of pairs where actual and predicted DISAGREE
df <- df %>% mutate(disagreement = ifelse(actual != predicted, 1, 0))
conc <- sum(df$agreement)
sum(df$disagreement)
conc <- c(vapply(ones$Predicted,
function(x) {
((x > zeros$Predicted))
},
FUN.VALUE=logical(nrow(zeros))))
# disc <- sum(c(vapply(ones$Predicted,
#                      function(x) {((x < zeros$Predicted))},
#                      FUN.VALUE = logical(nrow(zeros)))), na.rm = T)
concordance <- conc/nrow(df)
discordance <- disc/totalPairs
tiesPercent <- (1-concordance-discordance)
AUC = concordance + 0.5*tiesPercent
return(list("Concordance"=concordance, "Discordance"=discordance,
"Tied"=tiesPercent, "AUC"=AUC))
}
auc(actual, predicted)
actual <- ls_class
predicted <- ls_scr_class
AUC_calc <- function (actual, predicted){
df <- data.frame(actual = actual, predicted = predicted)
# Calculate total number of pairs to check - permutation of how many 1's and 0's exist in the actual column
totalPairs <- nrow(subset(df, actual == "1")) * nrow(subset(df, actual == "0"))
# Calculate concordance = number of pairs where actual and predicted AGREE
df <- df %>% mutate(agreement = ifelse(actual == predicted, 1, 0))
# Calculate discordance = number of pairs where actual and predicted DISAGREE
df <- df %>% mutate(disagreement = ifelse(actual != predicted, 1, 0))
conc <- sum(df$agreement)
sum(df$disagreement)
conc <- c(vapply(ones$Predicted,
function(x) {
((x > zeros$Predicted))
},
FUN.VALUE=logical(nrow(zeros))))
# disc <- sum(c(vapply(ones$Predicted,
#                      function(x) {((x < zeros$Predicted))},
#                      FUN.VALUE = logical(nrow(zeros)))), na.rm = T)
concordance <- conc/nrow(df)
discordance <- disc/totalPairs
tiesPercent <- (1-concordance-discordance)
AUC = concordance + 0.5*tiesPercent
return(list("Concordance"=concordance, "Discordance"=discordance,
"Tied"=tiesPercent, "AUC"=AUC))
}
auc(as.numeric(actual), as.numeric(predicted))
nrow(df_classif)
