---
title: "DATA 621 HW 1"
date: "Last edited `r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
  geometry: "left=0.5cm,right=0.5cm,top=1cm,bottom=2cm"
  html_document:
    df_print: paged
---

```{r, echo=FALSE, message=FALSE}
library(tidyr)
library(skimr)
library(dplyr)
library(knitr)
library(ggplot2)
library(corrplot)
# Set so that long lines in R will be wrapped:
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)

#Added library for kdepairs
library(ResourceSelection)
```

# **Business Analytics and Data Mining**
## Homework #1 Assignment Requirements{.tabset}

### **Overview** 
In this homework assignment, you will explore, analyze and model a data set containing approximately 2200 records. Each record represents a professional baseball team from the years 1871 to 2006 inclusive. Each record has the performance of the team for the given year, with all of the statistics adjusted to match the performance of a 162 game season.

Your objective is to build a multiple linear regression model on the training data to predict the number of wins for the team. You can only use the variables given to you (or variables that you derive from the variables
provided).

Below is a short description of the variables of interest in the data set:

<br>      

<table>


```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- "
| Variable Names         | Definition    					   	 | Theoretical Effect  	   |
|------------------------|:-------------------------------------:|------------------------:|
| INDEX 			     | Identification Variable (do not use)  | None 				   |
| TARGET_WINS 	         | Number of wins |   $12 			     |  					   |
| TEAM_BATTING_H 	     | Base Hits by batters (1B,2B,3B,HR)    | Positive Impact on Wins |
| TEAM_BATTING_2B 	     | Doubles by batters (2B) 				 | Positive Impact on Wins |
| TEAM_BATTING_3B 	     | Triples by batters (3B) 				 | Positive Impact on Wins |
| TEAM_BATTING_HR 	     | Homeruns by batters (4B) 			 | Positive Impact on Wins |
| TEAM_BATTING_BB 	     | Walks by batters 					 | Positive Impact on Wins |
| TEAM_BATTING_HBP	     | Batters hit by pitch (get a free base)| Positive Impact on Wins |
| TEAM_BATTING_SO 	     | Strikeouts by batters 				 | Negative Impact on Wins |
| TEAM_BASERUN_SB 	     | Stolen bases 						 | Positive Impact on Wins |
| TEAM_BASERUN_CS 	     | Caught stealing 						 | Negative Impact on Wins |
| TEAM_FIELDING_E 	     | Errors 								 | Negative Impact on Wins |
| TEAM_FIELDING_DP	     | Double Plays 						 | Positive Impact on Wins |
| TEAM_PITCHING_BB	     | Walks allowed 						 | Negative Impact on Wins |
| TEAM_PITCHING_H 	     | Hits allowed 						 | Negative Impact on Wins |
| TEAM_PITCHING_HR	     | Homeruns allowed 					 | Negative Impact on Wins |
| TEAM_PITCHING_SO	     | Strikeouts by pitchers 			 	 | Positive Impact on Wins |
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```
### **Deliverable:**

* A write-up submitted in PDF format. Your write-up should have four sections. Each one is described below. You may assume you are addressing me as a fellow data scientist, so do not need to shy away from technical details.
* Assigned predictions (the number of wins for the team) for the evaluation data set.
* Include your R statistical programming code in an Appendix.

### **Write Up:**

1. **DATA EXPLORATION (25 Points)**
Describe the size and the variables in the moneyball training data set. Consider that too much detail will cause a manager to lose interest while too little detail will make the manager consider that you aren’t doing your job. Some suggestions are given below. Please do NOT treat this as a check list of things to do to complete the assignment.
You should have your own thoughts on what to tell the boss. These are just ideas.

a. Mean / Standard Deviation / Median
b. Bar Chart or Box Plot of the data
c. Is the data correlated to the target variable (or to other variables?)
d. Are any of the variables missing and need to be imputed “fixed”?

2. **DATA PREPARATION (25 Points)**
Describe how you have transformed the data by changing the original variables or creating new variables. If you did transform the data or create new variables, discuss why you did this. Here are some possible transformations.
a. Fix missing values (maybe with a Mean or Median value)
b. Create flags to suggest if a variable was missing
c. Transform data by putting it into buckets
d. Mathematical transforms such as log or square root (or use Box-Cox)
e. Combine variables (such as ratios or adding or multiplying) to create new variables

3. **BUILD MODELS (25 Points)**
Using the training data set, build at least three different multiple linear regression models, using different variables (or the same variables with different transformations). Since we have not yet covered automated variable selection methods, you should select the variables manually (unless you previously learned Forward or Stepwise selection, etc.). Since you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done.
Discuss the coefficients in the models, do they make sense? For example, if a team hits a lot of Home Runs, it would be reasonably expected that such a team would win more games. However, if the coefficient is negative
(suggesting that the team would lose more games), then that needs to be discussed. Are you keeping the model even though it is counter intuitive? Why? The boss needs to know.

4. **SELECT MODELS (25 Points)**
Decide on the criteria for selecting the best multiple linear regression model. Will you select a model with slightly worse performance if it makes more sense or is more parsimonious? Discuss why you selected your model.
For the multiple linear regression model, will you use a metric such as Adjusted R2
, RMSE, etc.? Be sure to explain how you can make inferences from the model, discuss multi-collinearity issues (if any), and discuss other relevant model output. Using the training data set, evaluate the multiple linear regression model based on (a) mean squared error, (b) R2, (c) F-statistic, and (d) residual plots. Make predictions using the evaluation data set.

\newpage

## **Evaluation** 

### **Load the data**

```{r}
git_url<-
  "https://raw.githubusercontent.com/melbow2424/Data621_HW1/main/"

df_train <- 
  read.csv(paste0(git_url,"moneyball-training-data.csv"))

df_evaluation <- 
  read.csv(paste0(git_url,"moneyball-evaluation-data.csv"))
```

### 1. Data Exploration**

```{r}
print(skim(df_train))
```

Histograms a good way to visualize the distributions of the original variables.
```{r}
df_train %>% 
  gather(variable, value, TARGET_WINS: TEAM_FIELDING_DP)%>% #pivot longer to plot all variables
  ggplot(.,aes(x=value)) + #plotting every variable
  geom_density(fill = "darkgreen", alpha = 0.5) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()
```
##### Initial Observations
* The response variable __TARGET_WINS__ appears normally distributed
* Several variables such as __TEAM_BATTING_SO__ appear to be bimodal, which may resolve after the missing data is dealt with 
* Other variables like __TEAM_PITCHING_H__ appear to be skewed far left and may present a challenge unless imputation of missing values corrects this

#### **Get the Means of columns in Data**

```{r}
train_means<-sapply(df_train, function(x) round(mean(x, na.rm = TRUE)))
train_means
```
```{r}
eval_means<-sapply(df_evaluation, function(x) round(mean(x, na.rm = TRUE)))
eval_means
```
#### **Get the Medians of columns in data**

```{r}
train_medians<-sapply(df_train, function(x) round(median(x, na.rm = TRUE)))
train_medians
```
```{r}
eval_medians<-sapply(df_evaluation, function(x) round(median(x, na.rm = TRUE)))
eval_medians
```
#### **Replace NA values in columns with their respective Mean**
Made new copies of Train and Evaluation datasets, with imputed means.

```{r}
# Replace NA values in 'column_name' with 'mean'
df_train_mn <- df_train %>%
  mutate(TEAM_BATTING_SO =
           ifelse(is.na(TEAM_BATTING_SO),
                  train_means[8],TEAM_BATTING_SO))%>% 
  mutate(TEAM_BASERUN_SB = 
           ifelse(is.na(TEAM_BASERUN_SB),
                  train_means[9], TEAM_BASERUN_SB))%>%
  mutate(TEAM_BASERUN_CS =
           ifelse(is.na(TEAM_BASERUN_CS),
                  train_means[10], TEAM_BASERUN_CS))%>%
  mutate(TEAM_BATTING_HBP = 
           ifelse(is.na(TEAM_BATTING_HBP),
                  train_means[11],TEAM_BATTING_HBP))%>%
  mutate(TEAM_PITCHING_SO =
           ifelse(is.na(TEAM_PITCHING_SO),
                  train_means[15], TEAM_PITCHING_SO))%>%
  mutate(TEAM_FIELDING_DP =
           ifelse(is.na(TEAM_FIELDING_DP),
                  train_means[17], TEAM_FIELDING_DP))
```

```{r}
df_train_mn %>% 
  gather(variable, value, TARGET_WINS: TEAM_FIELDING_DP)%>% #pivot longer to plot all variables
  ggplot(.,aes(x=value)) + #plotting every variable
  geom_density(fill = "darkgreen", alpha = 0.5) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()
```
##### Observations after imputation with the mean
* The response variable __TARGET_WINS__ still appears normally distributed
* The bimodality of the __TEAM_BATTING__ variables is largely unresolved
* The far left skew of the __TEAM_PITCHING__  variables is largely unresolved  


```{r}
df_evaluation_mn <- df_evaluation %>%
  mutate(TEAM_BATTING_SO =
           ifelse(is.na(TEAM_BATTING_SO),
                  eval_means[8],TEAM_BATTING_SO))%>% 
  mutate(TEAM_BASERUN_SB = 
           ifelse(is.na(TEAM_BASERUN_SB),
                  eval_means[9], TEAM_BASERUN_SB))%>%
  mutate(TEAM_BASERUN_CS =
           ifelse(is.na(TEAM_BASERUN_CS),
                  eval_means[10], TEAM_BASERUN_CS))%>%
  mutate(TEAM_BATTING_HBP = 
           ifelse(is.na(TEAM_BATTING_HBP),
                  eval_means[11],TEAM_BATTING_HBP))%>%
  mutate(TEAM_PITCHING_SO =
           ifelse(is.na(TEAM_PITCHING_SO),
                  eval_means[15], TEAM_PITCHING_SO))%>%
  mutate(TEAM_FIELDING_DP =
           ifelse(is.na(TEAM_FIELDING_DP),
                  eval_means[17], TEAM_FIELDING_DP))
```


#### **Replace NA values with their respective Medians**
Made new copies of Train and Evaluation datasets, with imputed median.
```{r}
# Replace NA values in 'column_name' with 'median'
df_train_md <- df_train %>%
  mutate(TEAM_BATTING_SO =
           ifelse(is.na(TEAM_BATTING_SO),
                  train_medians[8],TEAM_BATTING_SO))%>%
  mutate(TEAM_BASERUN_SB =
           ifelse(is.na(TEAM_BASERUN_SB),
                  train_medians[9], TEAM_BASERUN_SB))%>%
  mutate(TEAM_BASERUN_CS =
           ifelse(is.na(TEAM_BASERUN_CS),
                  train_medians[10], TEAM_BASERUN_CS))%>%
  mutate(TEAM_BATTING_HBP =
           ifelse(is.na(TEAM_BATTING_HBP),
                  train_medians[11],TEAM_BATTING_HBP))%>%
  mutate(TEAM_PITCHING_SO =
           ifelse(is.na(TEAM_PITCHING_SO),
                  train_medians[15], TEAM_PITCHING_SO))%>%
  mutate(TEAM_FIELDING_DP =
           ifelse(is.na(TEAM_FIELDING_DP),
                  train_medians[17], TEAM_FIELDING_DP))
```


```{r}
df_train_md %>% 
  gather(variable, value, TARGET_WINS: TEAM_FIELDING_DP)%>% #pivot longer to plot all variables
  ggplot(.,aes(x=value)) + #plotting every variable
  geom_density(fill = "darkgreen", alpha = 0.5) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()
```

##### Observations after imputation with the median
* The response variable __TARGET_WINS__ still appears normally distributed
* The bimodality of the __TEAM_BATTING__ variables is largely unresolved
* The far left skew of the __TEAM_PITCHING__  variables is largely unresolved  

```{r}
df_evaluation_md <- df_evaluation %>%
  mutate(TEAM_BATTING_SO =
           ifelse(is.na(TEAM_BATTING_SO),
                  eval_medians[8],TEAM_BATTING_SO))%>% 
  mutate(TEAM_BASERUN_SB = 
           ifelse(is.na(TEAM_BASERUN_SB),
                  eval_medians[9], TEAM_BASERUN_SB))%>%
  mutate(TEAM_BASERUN_CS =
           ifelse(is.na(TEAM_BASERUN_CS),
                  eval_medians[10], TEAM_BASERUN_CS))%>%
  mutate(TEAM_BATTING_HBP = 
           ifelse(is.na(TEAM_BATTING_HBP),
                  eval_medians[11],TEAM_BATTING_HBP))%>%
  mutate(TEAM_PITCHING_SO =
           ifelse(is.na(TEAM_PITCHING_SO),
                  eval_medians[15], TEAM_PITCHING_SO))%>%
  mutate(TEAM_FIELDING_DP =
           ifelse(is.na(TEAM_FIELDING_DP),
                  eval_medians[17], TEAM_FIELDING_DP))
```

#### **Replace NA values with 0**
Made new copies of Train and Evaluation datasets, with imputed zeros.
```{r}
df_train_0 <- df_train %>%
  replace_na(list(
    INDEX = 0,
    TARGET_WINS = 0,
    TEAM_BATTING_H = 0,
    TEAM_BATTING_2B = 0,
    TEAM_BATTING_3B = 0,
    TEAM_BATTING_HR = 0,
    TEAM_BATTING_BB = 0,
    TEAM_BATTING_SO = 0,
    TEAM_BASERUN_SB = 0,
    TEAM_BASERUN_CS = 0,
    TEAM_BATTING_HBP = 0,
    TEAM_PITCHING_H = 0,
    TEAM_PITCHING_HR = 0,
    TEAM_PITCHING_BB = 0,
    TEAM_PITCHING_SO = 0,
    TEAM_FIELDING_E = 0,
    TEAM_FIELDING_DP = 0
  ))
```


```{r}
df_train_0 %>% 
  gather(variable, value, TARGET_WINS: TEAM_FIELDING_DP)%>% #pivot longer to plot all variables
  ggplot(.,aes(x=value)) + #plotting every variable
  geom_density(fill = "darkgreen", alpha = 0.5) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()
```

##### Observations after imputation with the zero
* Imputation with zero is a poor choice as it introduces strong peaks to the left of the distribution of many variables such as __TEAM_FIELDING_DP__ 

```{r}
df_evaluation_0 <- df_evaluation %>%
  replace_na(list(
    INDEX = 0,
    TARGET_WINS = 0,
    TEAM_BATTING_H = 0,
    TEAM_BATTING_2B = 0,
    TEAM_BATTING_3B = 0,
    TEAM_BATTING_HR = 0,
    TEAM_BATTING_BB = 0,
    TEAM_BATTING_SO = 0,
    TEAM_BASERUN_SB = 0,
    TEAM_BASERUN_CS = 0,
    TEAM_BATTING_HBP = 0,
    TEAM_PITCHING_H = 0,
    TEAM_PITCHING_HR = 0,
    TEAM_PITCHING_BB = 0,
    TEAM_PITCHING_SO = 0,
    TEAM_FIELDING_E = 0,
    TEAM_FIELDING_DP = 0
  ))
```

### Remove all rows with NA's

```{r}
df_train_rm<- na.omit(df_train)
df_evaluation_rm<- na.omit(df_evaluation)
```

```{r}
df_train_rm %>% 
  gather(variable, value, TARGET_WINS: TEAM_FIELDING_DP)%>% #pivot longer to plot all variables
  ggplot(.,aes(x=value)) + #plotting every variable
  geom_density(fill = "darkgreen", alpha = 0.5) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()
```
##### Observations after removal of missing data
* Although removal of missing data resolves the distributions of many variables, the sample size is reduced to under 10% of the original dataset from 2276 observations to only 191 observations in the training dataset

##### Given the loss of data with the removal of missing data, the most appropriet imputation is the mean and we should proceed with that method

```{r}
#print(skim(df_train_mn))
```

```{r}
#print(skim(df_train_md))
```

```{r}
#print(skim(df_train_0))
```

#### Correlation with response variable

```{r}
df_train_mn %>% 
  gather(variable, value, TEAM_BATTING_H: TEAM_FIELDING_DP)%>% #pivot longer to plot all variables
  ggplot(.,aes(x=value, y=TARGET_WINS)) + #plotting every variable
  geom_point(color = "grey", alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgreen") +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_classic()
```
##### Observations about correlations with TARGET_WINS
* None of the variable appear very strongly correlated with __TARGET_WINS__, but some the __TEAM_PITCHING__ variable have modest correlation
* Team batting variables appear to have no correlation with __TARGET_WINS__, with the exception of __TEAM_BATTING_H__ which has a modest positive correlation with wins

#### Checking for multicollinearity 

```{r}
df_train_mn %>% 
 select(-INDEX) %>% 
  cor(.,) %>% 
  corrplot(.,method = "ellipse", type = "lower", diag = FALSE)
```
##### Observations about correlations with TARGET_WINS
* The correlogram confirms that none of the variable are very strongly correlated with __TARGET_WINS__, with the exception of __TEAM_BATTING_H__ which has a modest positive correlation with wins
* Strong multicollinearity is seen between the following variable which needs to be taken into consideration when constructing the models: __TEAM_FIELDING_E__, __TEAM_PITCHING_HR__, __TEAM_BATTING_3B__, __TEAM_BATTING_HR__
* It may also not be advisable to include __TEAM_BATTING_HBP__ in the model because it has no correlation with wins or any other variable in the dataset

## **Models**

```{r}
model_initial <- lm(TARGET_WINS ~ TEAM_BATTING_H+TEAM_BATTING_2B +TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+
TEAM_BASERUN_SB+ TEAM_BASERUN_CS + TEAM_BATTING_HBP +TEAM_PITCHING_H+ TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+ TEAM_FIELDING_DP, data = df_train)

summary(model_initial)
```
```{r}
model_initial <- lm(TARGET_WINS ~ TEAM_BATTING_H+TEAM_BATTING_2B +TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+
TEAM_BASERUN_SB+ TEAM_BASERUN_CS + TEAM_BATTING_HBP +TEAM_PITCHING_H+ TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+ TEAM_FIELDING_DP, data = df_train_md)

summary(model_initial)
```

```{r}
# Keep only specific columns using subset()
df_1 <- subset(df_train_rm, select = c(TARGET_WINS , TEAM_BATTING_H , TEAM_BATTING_2B , TEAM_BATTING_3B, TEAM_BATTING_HR , TEAM_BATTING_BB, TEAM_BATTING_SO , TEAM_BASERUN_SB, TEAM_BASERUN_CS, TEAM_PITCHING_H, TEAM_PITCHING_HR, TEAM_PITCHING_BB, TEAM_FIELDING_E))

kdepairs(df_1)
```

```{r}
# Create a subset that includes all columns
all_columns_subset <- df_train_rm[, ]

kdepairs(all_columns_subset)
```



```{r}
model_initial <- lm(TARGET_WINS ~ TEAM_BATTING_H+TEAM_BATTING_2B +TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+
TEAM_BASERUN_SB+ TEAM_BASERUN_CS + TEAM_BATTING_HBP +TEAM_PITCHING_H+ TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+ TEAM_FIELDING_DP, data = df_train_mn)

summary(model_initial)
```
```{r}
# Initialize variables to store results
best_adjusted_r_squared <- -Inf
best_variable <- ""

# Loop through each independent variable
for (var in names(df_train_md)[-1]) {
  # Fit a linear model with the current variable
  lm_model <- lm(paste("TARGET_WINS  ~", var), data = df_train_md)
  
  # Get the Adjusted R-squared for the model
  adjusted_r_squared <- summary(lm_model)$adj.r.squared
  
  # Check if the current model has a higher Adjusted R-squared
  if (adjusted_r_squared > best_adjusted_r_squared) {
    best_adjusted_r_squared <- adjusted_r_squared
    best_variable <- var
  }
}


# Print the results
cat("Best independent variable:", best_variable, "\n")
cat("Best Adjusted R-squared:", best_adjusted_r_squared, "\n")
```
```{r}
# Initialize variables for forward selection
included_variables <- character(0)
best_adjusted_r_squared <- -Inf

while (length(included_variables) < length(names(df_train)) - 1) {
  remaining_variables <- setdiff(names(df_train)[-1], included_variables)
  adjusted_r_squared_values <- numeric(length(remaining_variables))
  
  for (i in seq_along(remaining_variables)) {
    # Fit a linear model with the included variables plus one candidate variable
    variables_to_include <- c(included_variables, remaining_variables[i])
    formula <- paste("TARGET_WINS ~", paste(variables_to_include, collapse = " + "))
    lm_model <- lm(formula, data = df_train)
    
    # Get the Adjusted R-squared for the model
    adjusted_r_squared <- summary(lm_model)$adj.r.squared
    adjusted_r_squared_values[i] <- adjusted_r_squared
  }
  
  # Find the index of the variable that gives the maximum increase in Adjusted R-squared
  best_variable_index <- which.max(adjusted_r_squared_values)
  
  # If the best Adjusted R-squared is greater than the current best, include the variable
  if (adjusted_r_squared_values[best_variable_index] > best_adjusted_r_squared) {
    best_adjusted_r_squared <- adjusted_r_squared_values[best_variable_index]
    included_variables <- c(included_variables, remaining_variables[best_variable_index])
  } else {
    # Break the loop if adding a variable doesn't improve the Adjusted R-squared
    break
  }
}

# Print the selected variables and their Adjusted R-squared
cat("Selected variables:", included_variables, "\n")
cat("Best Adjusted R-squared:", best_adjusted_r_squared, "\n")
```
```{r}
formula <- paste("TARGET_WINS ~", paste(included_variables, collapse = " + "))
model_initial2 <- lm(formula, data = df_train)

summary(model_initial)
```
```{r}
# Fit a multiple linear regression model using lm
model <- lm(TARGET_WINS ~ ., data = df_train)

# Summary of the regression model
summary(model)
```
```{r}
#Also tried to minus it as well. Did not work. 
df_train_with_combo <- df_train_rm %>%
  mutate(team_H = TEAM_BATTING_H + TEAM_PITCHING_H,
         team_HR = TEAM_BATTING_HR + TEAM_PITCHING_HR,
         team_BB = TEAM_BATTING_BB + TEAM_PITCHING_BB,
         team_SO = TEAM_BATTING_SO + TEAM_PITCHING_SO)%>%
  select(-TEAM_BATTING_H, -TEAM_PITCHING_H, -TEAM_BATTING_HR, -TEAM_PITCHING_HR, -TEAM_BATTING_BB,-TEAM_PITCHING_BB,
         -TEAM_BATTING_SO,- TEAM_PITCHING_SO)
```


```{r}
# Create a subset that includes all columns
all_columns_subset <- df_train_with_combo[, ]

kdepairs(all_columns_subset)
```
```{r}
# Fit a multiple linear regression model using lm
model <- lm(TARGET_WINS ~ ., data = df_train_with_combo)

# Summary of the regression model
summary(model)
```
```{r}
# Initialize variables for forward selection
included_variables2 <- character(0)
best_adjusted_r_squared <- 0

while (length(included_variables2) < length(names(df_train_with_combo)) - 1) {
  remaining_variables <- setdiff(names(df_train_with_combo)[-1], included_variables2)
  adjusted_r_squared_values <- numeric(length(remaining_variables))
  
  for (i in seq_along(remaining_variables)) {
    # Fit a linear model with the included variables plus one candidate variable
    variables_to_include <- c(included_variables2, remaining_variables[i])
    formula <- paste("TARGET_WINS ~", paste(variables_to_include, collapse = " + "))
    lm_model <- lm(formula, data = df_train_with_combo)
    
    # Get the Adjusted R-squared for the model
    adjusted_r_squared <- summary(lm_model)$adj.r.squared
    adjusted_r_squared_values[i] <- adjusted_r_squared
  }
  
  # Find the index of the variable that gives the maximum increase in Adjusted R-squared
  best_variable_index <- which.max(adjusted_r_squared_values)
  
  # If the best Adjusted R-squared is greater than the current best, include the variable
  if (adjusted_r_squared_values[best_variable_index] > best_adjusted_r_squared) {
    best_adjusted_r_squared <- adjusted_r_squared_values[best_variable_index]
    included_variables2 <- c(included_variables2, remaining_variables[best_variable_index])
  } else {
    # Break the loop if adding a variable doesn't improve the Adjusted R-squared
    break
  }
}

# Print the selected variables and their Adjusted R-squared
cat("Selected variables:", included_variables2, "\n")
cat("Best Adjusted R-squared:", best_adjusted_r_squared, "\n")
```
```{r}
formula <- paste("TARGET_WINS ~", paste(included_variables2, collapse = " + "))
model_initial2 <- lm(formula, data = df_train_with_combo)

summary(model_initial)
```
```{r}
model_initial <- lm(TARGET_WINS ~ team_H + team_BB + TEAM_FIELDING_E +TEAM_FIELDING_DP+ team_SO+ team_HR+ TEAM_BATTING_HBP+ TEAM_BATTING_3B +TEAM_BASERUN_SB, data = df_train_with_combo)

summary(model_initial)
```
```{r}
df_2 <- subset(df_train_with_combo, select = c(TARGET_WINS, team_H, team_BB, TEAM_FIELDING_E, TEAM_FIELDING_DP, team_SO, team_HR, TEAM_BATTING_HBP, TEAM_BATTING_3B, TEAM_BASERUN_SB))

kdepairs(df_2)
```

### Reference

* "Pythagorean Theorem of Baseball." Baseball Reference, https://www.baseball-reference.com/bullpen/Pythagorean_Theorem_of_Baseball. Accessed 11 September 2023.
* No author listed. "Pythagorean Expectation in Major League Baseball." Digital Commons @ Cal Poly, https://digitalcommons.calpoly.edu/cgi/viewcontent.cgi?article=1067&context=statsp. Accessed 11 September 2023.